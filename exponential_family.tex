\documentclass{article} % For LaTeX2e


%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{/Users/miw267/Repos/latex_preamble/preamble}


%%% CONTROL SIZE OF BLOCK MATRICES
% Reference: https://tex.stackexchange.com/questions/14071/how-can-i-increase-the-line-spacing-in-a-matrix

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother
 
%%% MARKOV RANDOM FIELDS 
\newcommand{\selfpotential}{\phi}
\newcommand{\edgepotential}{\psi}

%%% HIDDEN MARKOV MODEL
\newcommand{\state}{x}

%%% BAYESIAN LINEAR REGRESSION
\newcommand{\new}{\text{new}}
\newcommand{\betaML}{\wh{\+\beta}_{\text{ML}}}


\begin{document}

%%% UNTOGGLE FOR PUBLIC RELEASE
\renewcommand{\redfootnote}[1]{}
\renewcommand{\red}[1]{} 
\renewcommand{\blue}[1]{} 


\title{Exponential Families} 

\maketitle
\tableofcontents
\newpage 

\section{Exponential Families} \label{sec:ef}

We are interested in  exponential families primarily because they makes inference easier.   When a problem can be cast within the exponential family framework,  inference can be tied to general principles,  and parameter updates often have nice interpretations.    This is true regardless of whether we're doing frequentist inference (such as maximum likelihood) or Bayesian inference.    Bayesian inference with exponential family likelihoods tends to be especially nice,  as all exponential family likelihoods have conjugate priors,  and distributions with conjugate priors are often also exponential families \cite{bernardo2009bayesian}. \redfootnote{TODO:  Get clearer on the relationship.  There is a brief discussion on this in \cite{bernardo2009bayesian}.  An answer from mathstackexchange at \url{https://stats.stackexchange.com/questions/176668/can-anyone-explain-conjugate-priors-in-simplest-possible-terms} gives a promising quote, although I do not know the source: ``Outside this exponential family setting, there is no non-trivial family of distributions with a fixed support that allows for conjugate priors. This is a consequence of the Darmois-Pitman-Koopman lemma." }   More complicated models may not be exponential families,  but may have exponential family complete conditional distributions;  in such situation,  we can appeal to exponential family formalisms to more easily work out inference schemes for expectation maximization,  variational inference,  or Gibbs sampling.   

\subsection{Definitions}

\input{inputs/definition_exponential_family}

\begin{remark}
Observe \cite{chua2019stats} that without loss of generality, we can 
\begin{alphabate} 
\item Set $h(x) \equiv 1$ if we change $\mu$ to $\wt{\mu}$, chosen so that its Radon-Nikodym derivative with respect to $\mu$ is $h$.\footnote{That is, we can always absorb $h$ into $\mu$ and write \eqref{eqn:exponential_family_natural} as  $p_\eta(x) = \exp \{ \eta^T t(x) - a(\eta)\}$, in which case \eqref{eqn:partition_function} becomes $Z(\eta) := \ds\int  \exp \{ \eta^T t(x)  \} \; \wt{\mu} (dx) $ where $\wt{\mu}(A) := \ds\int_A h d\mu$ for any measurable set $A$. }
\item \label{rk:nonuniqueness_of_natural_parameter} \textit{(Non-uniqueness of natural parameter)} Assume $M \in \R^{p \times p}$ is invertible, and define $\wt{t} = M t(x)$. Then defining $\wt{\eta} = M^{-1} \eta$ results in an equivalent family.  As a special case, $\eta$ could be multiplied by a non-zero constant $c$ if $t(x)$ is divided by $c$.  Thus, a natural parameterization is not unique; we should speak of \textit{a} natural parameter,  rather than \textit{the} natural parameter.
\item Change $h$ to $\wt{h}(x) = p_0(x)$, assuming that $0 \in H$.  Then, changing $a(\eta)$ to $\wt{a}(\eta) = a(\eta) - a(0)$ results in an equivalent family. 
\end{alphabate}
\label{rk:alternate_constructions_for_the_exponential_family}
\end{remark}


\begin{definition}
The \textit{natural parameter space} is the set of parameters $\eta$ for which the integral \eqref{eqn:partition_function} is finite; i.e., it is $H:=\set{\eta : Z(\eta) < \infty}$.
\end{definition}

\begin{definition}
An exponential family is said to be \textit{regular} if the natural parameter space is an open set.
\end{definition}

One can \textit{reparameterize} a regular exponential family with some other coordinates $\theta$.  If one writes the natural parameter as a continuous function $\eta(\theta)$, then the density \eqref{eqn:exponential_family_natural} becomes
\begin{align}
\label{eqn:exponential_family}
 p_\theta(x) = h(x) \exp \{ \eta(\theta)^T t(x) - a(\eta(\theta))\} 
 \end{align}
 The reparameterized family is regular as well, since $\Theta := \eta^{-1}(H)$ is open. 


\begin{remark}
Exponential family members can have intractable normalization constants.  Consider, for example,  the Ising model.  See pp. 3 of \cite{taylor2013multiparameter}.
\end{remark}


\begin{definition}
An exponential family is said to be \textit{minimal} if the components of the sufficient statistics $t(x)$ are linearly independent ($\mu$-a.e.).\redfootnote{CHECK: This statement, when given by David Blei, made no mention of almost everywhere.  The next statement, however, which came from \cite{johnson2016composing}, does.  I attempted to align them by adding ``almost everywhere" to the linear independence claim.  Hopefully this is valid.}  That is, there must be no $\eta(\theta) \in \R^n \textbackslash \set{0}$ such that $\eta(\theta)^T t(x) = 0$ ($\mu$-a.e.).\redfootnote{Is it strictly speaking necessary to assume that the parameters are real-valued? If so, why?}
\end{definition}

  An example of a non-minimal exponential family is the categorical distribution (Example \ref{ex:categorical_as_ef}).\redfootnote{Justify, and state how to rectify. Also demonstrate the importance of this.} 


\subsection{Examples}

Here we give examples of exponential families, showing how to derive the exponential family forms.   Note that the result of such computations are readily available for a wide variety of exponential family members (see e.g. \cite{nielsen2009statistical} or the Wikipedia page on exponential families). 

 
\subsubsection{Categorical distribution}

\begin{example}{(Categorical Distribution)}
\label{ex:categorical_as_ef} We can write the density of the categorical distribution in exponential family form. 	 Given one-hot encoded observations $x \in \set{0,1}^K$ and simplex-valued parameter $\pi \in \Delta_{K-1}$, we can write
\[p(x \cond \pi) = \prod_{k=1}^K \pi_k^{x_k} = \exp \{ \sum_{k=1}^K x_k \log \pi_k\} \]
 with natural parameter, $\eta(\pi) = \log \pi$, the sufficient statistics $t(x) = x$, carrier density $h(x)=1$ and log normalizer $0$.
\end{example}

\subsubsection{Dirichlet distribution}
\begin{example}{(Dirichlet Distribution)} 
\label{ex:dirichlet_as_ef} We can write the density of the Dirichlet distribution in exponential family form:
\begin{align*}
p (\pi \cond\alpha) &= \df{\Gamma(\sum_k \alpha_k)}{\prod_k \Gamma (\alpha_k) } \pi_1^{\alpha_1 -1} \cdot \cdot \cdot \pi_K^{\alpha_K -1} \\
&= \exp \bigg\{ \sum_{k=1}^K (\alpha_k -1) \log \pi_k - \bigg[ \sum_k \log \Gamma (\alpha_k)-  \log \Gamma (\sum \alpha_k) \bigg]  \bigg\}
\end{align*}
with natural parameter $\eta(\alpha) = [\alpha_1 -1, ..., \alpha_K -1]^T$, sufficient statistics $t(\pi) = \log \pi = [\log \pi_1, ..., \log \pi_K]^T$, carrier density $h(\pi)=1$, and log normalizer $a(\alpha) =  \sum_k \log \Gamma (\alpha_k ) - \log \Gamma (\sum_k \alpha_k)$. 
  
\end{example} 

For an example of how the natural parametrization can help provide insight into message passing,  see Section \ref{sec:mvn_in_message_passing}.

\begin{remark} The exponential family representation of the Dirichlet, as given in Example \ref{ex:dirichlet_as_ef}, is useful when we want to compute the expectation of a log probability from a Dirichlet distributed probability vector (as happens in the derivation of LDA with variational inference; see my notes on variational inference).  

In those notes,  we see
\begin{align} 
\E [ \log \pi_k] &= \E [t_k(p)] \stackrel{1}{=} \df{\partial}{\partial \eta_k} a(\eta) \nonumber \\
&=\Psi(\alpha_k) -  \Psi(\sum_k  \alpha_k) \label{eqn:expectation_of_log_probability} 
\end{align}
where (1) uses a well-known exponential family property (see Proposition \ref{prop:expected_sufficient_statistic_as_derivative_of_log_normalizer}) and where $\Psi(\cdot)$ is the first derivative of the $\log \Gamma$ function.   It is known as the \textit{digamma function}.  $ $

% See e.g., https://zhiyzuo.github.io/Exponential-Family-Distributions/
\end{remark}

\subsubsection{Truncated normal distribution}

\begin{example}{(Truncated normal distribution)} 
\label{ex:truncated_normal_as_ef} The univariate truncated normal distribution $\TruncatedNormal(\mu,  \sigma^2,  \Omega)$ results when a normal distribution $\N(\mu,  \sigma^2)$ is truncated to some set $\Omega \in \R$. \footnote{For more information on the truncated normal,  see e.g.  \cite{burkardt2014truncated} or \url{http://parker.ad.siu.edu/Olive/ch4.pdf}.}   Note that the parameters $\mu, \sigma^2$ denote the mean and variance of the \textit{parent} normal distribution;  i.e.  if $X \sim \TruncatedNormal(\mu,  \sigma^2,  \Omega)$ then $\E[X] \neq \mu$ (unless $\Omega = \R$). 

If we assume that the truncation set is an interval $\Omega = (a,b)$ for $a,b \in \R$,  then the distribution $\TruncatedNormal(\mu,  \sigma^2,  (a,b))$ has p.d.f.

\begin{align*}
f(x ; \mu,  \sigma^2,  a,  b) = \df{\phi_{\mu, \sigma^2} (x) }{\Phi_{\mu, \sigma^2} (b)  - \Phi_{\mu, \sigma^2} (a) } \indicate{a \leq x \leq b} \labelit \label{eqn:pdf_of_normal_truncated_to_an_interval}
\end{align*} 

where $\phi_{\mu,  \sigma^2}$ and $\Phi_{\mu,  \sigma^2}$ denote the pdf and cdf,  respectively,  of a univariate normal distribution with mean $\mu$ and variance $\sigma^2$.   

If we write
\begin{align*}
f(x ; \mu,  \sigma^2,  a,  b) & = K \df{1}{\sqrt{2 \pi \sigma^2}} \exp \bp{-\half \df{(x-\mu)^2}{\sigma^2}}  \indicate{a \leq x \leq b} \\
& = K \df{1}{\sqrt{2 \pi}} \exp \bp{ - \frac{1}{2 \sigma^2} x^2 + \frac{\mu}{\sigma^2} x  + \frac{\mu^2}{\sigma^2}  - \log \sigma}  \indicate{a \leq x \leq b} 
\end{align*} 
where $K := \big( \Phi_{\mu, \sigma^2} (b)  - \Phi_{\mu, \sigma^2} (a) \big)^{-1}$,  then we see that see that  $\TruncatedNormal(\mu,  \sigma^2,  (a,b))$ belongs to the exponential family \eqref{eqn:exponential_family} where,  in this case,  we have natural parameter $\eta = (\frac{1}{\sigma^2},  \frac{\mu}{\sigma^2})^T$,  sufficient statistics function $t(x) = (-\half x^2,  x)^T$,  carrier density $h(x) =  \frac{1}{\sqrt{2 \pi}}  \indicate{a \leq x \leq b}$,  and log normalizer $a(\theta) = \log K + \frac{\mu^2}{\sigma^2} - \log \sigma$.
 
\end{example}

\begin{remark}
The truncated normal distribution differs from the normal distribution only in its carrier density $h(x)$ and therefore log normalizer $a(\theta)$.  The natural parameter $\eta$ and sufficient statistics function $t(x)$ are identical.   Thus,  knowing $\eta$ and $t(x)$ is not sufficient to determine the form of the probability distribution.
\label{rk:truncated_normal_differs_from_normal_only_in_terms_of_carrier_density}
\end{remark}


\subsubsection{Inverse Gamma distribution}

\begin{example}{(Inverse Gamma Distribution)} 
\label{ex:inverse_gamma_as_ef} The Inverse Gamma distribution is the distribution of the reciprocal of a Gamma random variable.\footnote{The density of the inverse gamma can easily be obtained from the gamma density by defining the transformation $Y = \frac{1}{X} := g(X)$ and then applying the change of variables formula,  $f_Y(y) = f_X (g^{-1}(y)) | \df{d}{dy} g^{-1}(y)|$.}  We can write the density of the Inverse Gamma $\InverseGamma(\alpha,  \beta)$ distribution in exponential family form:
\begin{align*}
p (x \cond \alpha,  \beta) &= \df{\beta^\alpha}{ \Gamma (\alpha) } x^{-\alpha -1} \exp \bigg(- \df{\beta}{x} \bigg) \\
&= \exp \bigg\{ (-\alpha - 1) \log x + (- \beta) \frac{1}{x} + \log \df{\beta^\alpha}{\Gamma(\alpha)} \bigg\}
\end{align*}
with natural parameter $\eta(\alpha) = [-\alpha-1,  -\beta]^T$, sufficient statistics $t(x) = [\log x ,  \frac{1}{x}]^T$, carrier density $h(x)=1$, and log normalizer $a(\alpha,  \beta) =  \log \frac{\beta^\alpha}{\Gamma(\alpha)} $. 
  
\end{example} 

\subsubsection{Multivariate normal}

\begin{example}{(Multivariate normal)} 
\label{ex:mvn_as_ef}
We can write the density of a multivariate normal $\N(\+\mu, \+\Sigma)$ distribution in exponential form
\begin{align*}
	p(\+x \cond \+\mu, \+\Sigma) &= (2\pi)^{-d/2} |\+\Sigma |^{-1/2} \exp \bigg\{ -\half (\+x - \+\mu)^T \+\Sigma^{-1} (\+x - \+\mu) \bigg\}  \\
& \stackrel{1}{=} (2\pi)^{-d/2} \exp \bigg\{ -\half \explaintermbrace{$-\half \vectorize (\+\Sigma^{-1})^T \vectorize (\+x\+x^T) $}{\+x^T \+\Sigma^{-1} \+x} + \+x^T \+\Sigma^{-1} \+\mu -\half \+\mu^T \+\Sigma^{-1} \+\mu + \half \log |\+\Sigma^{-1} | \bigg\} \\
	\labelit \label{eqn:mvn_as_ef}
\end{align*} 
with natural parameter $\eta(\+\mu, \+\Sigma) = \big(-\half \vectorize (\+\Sigma^{-1}) , \,  \+\Sigma^{-1} \+\mu \big) $, sufficient statistics $t(\+x) = \big(\vectorize (\+x \+x^T), \+x \big)$, carrier density $h(x) = (2\pi)^{-d/2}$ and log normalizing $a(\+\mu, \+\Sigma) =  -\half \+\mu \+\Sigma^{-1} \+\mu + \half \log |\+\Sigma^{-1} | $. 
\end{example}

\begin{remark}
From Example \ref{ex:mvn_as_ef}, we see that the natural parameters of the MVN are the \textit{precision} $\+\Sigma^{-1}$ and \textit{precision-weighted mean} $\+\Sigma^{-1} \+\mu$. 
\end{remark}

\begin{remark}
 The underbrace representation in Equation (1) is given by $\+x^T \+\Sigma^{-1} \+x = \tr (\+x^T \+\Sigma^{-1} \+x) = \tr (\+\Sigma^{-1} \+x \+x^T) =\vectorize (\+\Sigma^{-1})^T \vectorize (\+x\+x^T)$.\footnote{Recall $\tr(\+A\+B) = \vectorize (\+A)^T \vectorize (\+B)$.}  
\end{remark}

\begin{remark}
In Section \ref{sec:normal_data_with_non_conjugate_prior}, we use the exponential family representation to derive the updates to the mean for a Bayesian normal model with conditionally conjugate prior.
\end{remark}

\begin{remark}
\label{rk:mvn_from_ef}
Equation \eqref{eqn:mvn_as_ef} also says that if a random vector $\+x$ has a density on $\R^d$ that is proportional to $\exp \{ -\half \+x^T \+A \+x + \+x^T \+b\}$ for some matrix $\+A$ and vector $\+b$, then $\+x$ must be multivariate normal with covariance $\+A^{-1}$ and mean $\+A^{-1}\+b$. 
\end{remark}
	

\subsubsection{Inverse Wishart distribution}

\begin{example}{(Inverse Wishart distribution)} 
\label{ex:inverse_wishart_as_ef} The Inverse Wishart distribution (Section \ref{sec:inverse_wishart_distribution}) is the distribution of the inverse of a Wishart random variable.   We can write the density of the Inverse Wishart $\InverseWishart(\+\Psi,  \nu)$ distribution in exponential family form:
\begin{align*}
p (\+X  \cond \+\Psi,  \nu) & \stackrel{1}{=} C(\+\Psi,  \nu) \; | \+X | ^{-(\nu + p + 1)/2} \exp \bigg\{ -\half \tr (\+\Psi \+X^{-1} ) \bigg\} \\
& = \exp \bigg\{ \frac{-(\nu + p + 1)}{2}  \log | \+ X | \;  -\half \tr (\+\Psi \+X^{-1} )  \; + \log  C(\+\Psi,  \nu)   \bigg\} \\
& \stackrel{2}{=} \exp \bigg\{ \frac{-(\nu + p + 1)}{2}  \log | \+ X | \;  -\half  \ds\sum_{i,j=1}^p \+\Psi_{ij} \+X^{-1}_{ij} \; + \log  C(\+\Psi,  \nu)   \bigg\} 
\end{align*}
Equation (1) gives the standard representation of the $\InverseWishart(\+\Psi,  \nu)$ density,  where   $C(\+\Psi,  \nu)$ is the normalizing constant,  $| \cdot |$ refers to the determinant,  $\+X,  \+\Psi \in \R^{p \times p}$ are positive definite matrices,  and $\nu > p-1$.    Equation (2) uses the fact that the trace of a matrix product behaves like a dot product \eqref{eqn:trace_of_matrix_product}.      


As we see from the last line,  in the exponential family representation,  we have natural parameter $\eta = [\frac{-(\nu + p + 1)}{2},  \; -\half \vectorize (\+\Psi)]^T$, sufficient statistics $t(\+X) = [\log | \+X | , \vectorize (\+X^{-1})]^T$, carrier density $h(\+X)=1$, and log normalizer $ \log  C(\+\Psi,  \nu)$.   
  
\end{example} 

%{eqn:trace_of_matrix_product}

\subsubsection{Hidden Markov Models} 

\begin{example}{(Hidden Markov Models)} 
A  hidden Markov model (HMM) is a tool for representing probability distributions over sequences of observations.  The HMM assumes that the observation at time $t$ was generated by some process whose state $\state_t$ is hidden from the observer.  Moreover, it assumes that the sequence of states satisfies the \textit{Markov property}:  conditional on the current state $\state_t$, its future and past hidden states are independent.  Finally, there is a Markov property on outputs:  conditional on the current state $\state_t$, the output $y_t$ is independent of all other hidden states and outputs.\redfootnote{I might have lifted this paragraph overviewing HMM's from somewhere; check into that.}

% Note: No need to introduce the Bayesian hidden markov model here; we're just showing that the complete data likelihood is in the
% exponential family.
%
%The full generative model over the parameters, hidden state sequence $\state_{1:T}$ and observation sequence $y_{1:T}$ is
%
%\begin{align*}
%\pi &\sim \text{Dir}(\alpha^\pi) & A_k &\iid \text{Dir}(\alpha^{A_k})  & \phi_k &\iid p(\phi_k \cond \beta) \\
%\state_1 &\sim \pi & \state_{t+1} &\sim A_{\state_t} & y_t &\sim p(y_t \cond \phi_{z_t})
%\end{align*}
 
The the \textit{complete data likelihood} for the HMM is given by

\begin{align}
 p(\state_{1:T}, y_{1:T} \cond \theta) &=  p(\state_1 \cond \theta)  p(y_1 \cond \state_1, \theta) \ds\prod_{t=2}^T p(\state_t \cond \state_{t-1}, \theta) p(y_t \cond \state_t, \theta) \nonumber \\
 &=  p(\state_1 \cond \pi) p(y_1 \cond \state_1, \phi) \ds\prod_{t=2}^T p(\state_t \cond \state_{t-1}, A) p(y_t \cond \state_t, \phi)   \nonumber \\
 &= \pi_{\state_1}  \; \ds\prod_{t=2}^T A_{\state_{t-1}, \state_t} \ds\prod_{t=1}^T p(y_1 \cond \phi_{\state_t}) \label{eqn:hmm_cdl_compact}
 \end{align}
 
where we have defined 

\begin{itemize}
\item $y_{1:T}=(y_1, ..., y_T)$ observed sequence
\item $\state_{1:T} =(\state_1, ...., \state_T)$: hidden state sequence ($\state_t \in \{1,...,K \}$) 
\item $\pi = \{ \pi_k \}, \pi_k = P(\state_1 = k)$: initial state distribution
\item $A=\{A_{kk'}\}, A_{kk'} = P(\state_t= k' \cond \state_{t-1}=k):$ state transition probability matrix 
\item $\phi = (\phi_k)_{k=1}^K$ a set of parameters, each governing an output distribution (also called emissions distribution) associated to each hidden state; that is, $ P(y_t \cond \state_t=k) = P(y_t \cond \phi_k)$. 
\item $\theta = (\pi, A, \phi)$: model parameters
\end{itemize}


We can write the complete data likelihood \eqref{eqn:hmm_cdl_compact} as
\begin{align}
p(\state_{1:T}, y_{1:T} \cond \theta) &=  \exp\bigg \{ \log p(\state_1 \cond \pi) + \ds\sum_{t=2}^T \log p(\state_t \cond \state_{t-1}, A) +  \ds\sum_{t=1}^T \log p(y_t \cond \state_t, \phi) \bigg\}  \nonumber \\
&= \exp \bigg \{ \log \pi_{\state_1}  + \ds\sum_{t=2}^T \log A_{\state_{t-1}, \state_t} + \ds\sum_{t=1}^T \log p(y_1 \cond \phi_{\state_t}) \bigg\} \nonumber \\
&=  \exp\bigg \{  \ds\sum_{k=1}^K \state_1^k \log \pi_k + \ds\sum_{t=2}^T \ds\sum_{k, k'=1}^K \state_{t-1}^k \state_t^{k'} \log A_{kk'} + \ds\sum_{t=1}^T \ds\sum_{k=1}^K \state_t^k \log p(y_t \cond \phi_k)  \bigg\} \label{eqn:hmm_ef_form}
\end{align}

where we have defined
\[ \state_t^k =  
\begin{cases}	  
1, & \text{if the latent state at time $t$ is $k$} \\
0, & \text{otherwise}
\end{cases} \]

and \eqref{eqn:hmm_ef_form} shows that the HMM is an exponential family, so long as the emissions distributions are. 
The sufficient statistics for $\log \pi_k$ are $\state_1^k$, and the sufficient statistics for $\log A_{kk'}$ are $\sum_{t=2}^T \state_{t-1}^k \state_t^{k'}$. 

\end{example}

\subsubsection{Non-examples}

Some non-examples include 
\begin{itemize}
\item The Cauchy distribution (since, as we will see in Remark \ref{rk:any_exponential_family_has_finite_moments}, any exponential family must have finite moments)
\item The uniform distribution, whose density cannot be written in the form \eqref{eqn:exponential_family}. %(Loosely speaking, its carrier density is a function of the parameter.)
\end{itemize}


\subsection{Properties}

\subsubsection{Relationship between moments and the normalizer function}
\begin{proposition}
Let $X$ have an exponential family distribution with natural parameter $\eta$, sufficient statistics function $t$, and log normalizer function $a$.   Then  
\begin{align*}
\nabla a(\eta) = \E[t(X)] 
\labelit \label{eqn:expected_sufficient_statistic_as_derivative_of_log_normalizer}	
\end{align*}

\begin{proof}

Since $X$ is in the exponential family, its density can be written in the form
 \[ p(x \cond \eta) =  \exp \{ \eta^T t(x) - a(\eta) + \log h(x)\} \] 
where 
\[a(\eta) = \log \int_{\mathcal{X}} \exp \{ <t(x), \eta> + \log h(x) \} \; d\nu_{\mathcal{X}} \] 

Thus
\begin{align*}
\nabla a (\eta) & \stackrel{1}{=}  \df{ \int_{\mathcal{X}} t(x)  \exp \{ <t(x), \eta> + \log h(x) \} \; d\nu_{\mathcal{X}} }{\int_{\mathcal{X}} \exp \{ <t(x), \eta> + \log h(x) \} \; d\nu_{\mathcal{X}} } \\
& \stackrel{2}{=} \int_{\mathcal{X}} t(x)  \exp \{ <t(x), \eta> - a(\eta) + \log h(x) \} \; d\nu_{\mathcal{X}}  \\
& = \int_{\mathcal{X}} t(x)  \; p(x \cond \eta) \; d\nu_{\mathcal{X}}  \\
&= \E[t(X)]
\end{align*}

where in  Equation (1) we take the derivative of a logarithm (interchanging the gradient and the integral), and in Equation (2) we recognize the denominator as $\exp a(\eta)$.	
\end{proof}

\label{prop:expected_sufficient_statistic_as_derivative_of_log_normalizer}
\end{proposition}

\begin{task}
Justify formally the interchange of gradient and integral in Proposition \ref{prop:expected_sufficient_statistic_as_derivative_of_log_normalizer}.	
\end{task}

\begin{remark}
In a manner similar to that of Proposition \ref{prop:expected_sufficient_statistic_as_derivative_of_log_normalizer}, we can show that the covariance matrix of the sufficient statistics is the Hessian of the log-normalizer calculated at its natural parameter:
\[ \Cov[t(X)] = \nabla^2 a(\eta) \]	

In fact, all moments of an exponential family are finite (recall from Definition \ref{def:exponential_family} that exponential family membership requires $a$ to be a $C^\infty$ function).  This explains why the Cauchy distribution (of undefined mean) is not an exponential family.
\label{rk:any_exponential_family_has_finite_moments}
\end{remark}



For more information on Proposition \ref{prop:expected_sufficient_statistic_as_derivative_of_log_normalizer}, see \cite{jordan2010ef}, \cite{nielsen2010entropies}, or \cite{nielsen2009statistical}.   

\subsubsection{Entropy, cross-entropy, and KL divergence}

We can provide a closed-form expression for the KL divergence between two members of the same exponential family. 

\begin{proposition}
Consider two probability distributions from the same exponential family with density $p$, and and let their natural  parameters denoted $\eta$ and $\wt{\eta}$, respectively.  Then the KL-divergence (i.e. relative entropy) is given by

\begin{align*}
\texttt{KL}(\eta || \wt{\eta}) = \bigip{\nabla a(\eta), \eta - \wt{\eta}} + a(\eta) - a(\eta) 
\labelit \label{eqn:KL_divergence_between_two_members_of_the_sample_exponential_family}
\end{align*}
 


\begin{proof}
We assume for simplicity of notation (but without loss of generality) that $\mu$ in Definition \ref{def:exponential_family} is the Lesbesgue measure. 

\begin{align*}
\texttt{KL}(\eta || \wt{\eta}) &= \ds\int p(x \cond \eta) \log \bigg( \df{p(x \cond \eta)}{p(x \cond \wt{\eta})}\bigg) \;dx \\ 
& = \ds\int p(x \cond \eta) \; \bigg[ \bigip{t(x), \eta - \wt{\eta}}  + a(\wt{\eta}) - a(\eta) \bigg] \; dx \\
&= \bigip{\E_{\eta}[t(X)],  \eta - \wt{\eta} } + a(\wt{\eta}) - a(\eta) \\
& \stackrel{\eqref{eqn:expected_sufficient_statistic_as_derivative_of_log_normalizer}}{=}  \bigip{\nabla a(\eta), \eta - \wt{\eta} } + a(\wt{\eta}) - a(\eta)
\end{align*}
	
\end{proof}
\label{prop:kl_divergence_between_members_of_same_exponential_family}
\end{proposition}

By reasoning in a similar way as the proof of Proposition \ref{prop:kl_divergence_between_members_of_same_exponential_family}, expressions for the entropy $\H[\eta] = -\E_{\eta}[\log p(\eta)]$ and cross-entropy $\H[\eta, \wt{\eta}] = -\E_{\eta}[\log p(\wt{\eta})]$ can also be provided:

\begin{subequations}
\begin{align}
\H[\eta]&= a(\eta) - \bigip{\eta, \nabla a(\eta)} - \E_{\eta} [\log h(X)]	\label{eqn:entropy_of_an_exponential_family}\\
\H[\eta, \wt{\eta}] &=  a(\wt{\eta}) - \bigip{\wt{\eta}, \nabla a(\eta)} - \E_{\eta}  [\log h(X)] \label{eqn:cross_entropy_between_two_exponential_family_members}
\end{align}
\label{eqn:entropy_and_cross_entropy_between_two_exponential_family_members}
\end{subequations}



Note that unlike with KL divergence \eqref{eqn:KL_divergence_between_two_members_of_the_sample_exponential_family}, the expressions for entropy and cross entropy \eqref{eqn:entropy_and_cross_entropy_between_two_exponential_family_members} may not have a closed form solution.  However, note that these expressions will always automatically have closed form solution when the carrier density satisfies $h(x) \equiv 1$, as is the case, for example, with the Gaussian, Dirichlet, and inverse gamma distributions.\footnote{Some presentations give $h(x)=C$ for some constant $C$, but note that the constant can simply be absorbed into the log-normalizer $a(\eta)$.}. In that case, we have 

\begin{subequations}
\begin{align}
\H[\eta]&= a(\eta) - \bigip{\eta, \nabla a(\eta)} \label{eqn:entropy_of_an_exponential_family_with_base_measure_unity}\\
\H[\eta, \wt{\eta}] &=  a(\wt{\eta}) - \bigip{\wt{\eta}, \nabla a(\eta)}  \label{eqn:cross_entropy_between_two_exponential_family_members_with_base_measure_unity}
\end{align}
\label{eqn:entropy_and_cross_entropy_between_two_exponential_family_members_with_base_measure_unit}
\end{subequations}


% NOT EXACTLY Note that the cross-entropy formula \eqref{eqn:cross_entropy_between_two_exponential_family_members} still applies even when the two distributions are \textit{different} exponential families!

\begin{example}
Let us apply \eqref{eqn:entropy_of_an_exponential_family} to compute the entropy of a centered Laplace distribution, which has pdf
\[ f(x \cond \sigma) = -\frac{1}{2 \sigma} \exp \bigg\{ - \frac{|x|}{\sigma}\bigg\}  \]	
The natural parameter is $\eta = - \frac{1}{\sigma}$, the log normalizer is $a(\eta) = \log (-\frac{2}{\eta})$ (and so $\nabla a(\eta) = -\frac{1}{\eta}$), and the carrier density is $h(x)=1$ (which allows us to use \eqref{eqn:entropy_of_an_exponential_family_with_base_measure_unity} instead of \eqref{eqn:entropy_of_an_exponential_family}).    
Using this, we can easily compute 
\begin{align*}
\H[\eta]&= a(\eta) - \bigip{\eta, \nabla a(\eta)} = \log \bigg(-\frac{2}{\eta} \bigg)  - \bigip{\eta, - \frac{1}{\eta}}  =\log \bigg(-\frac{2}{\eta} \bigg) + 1 \\ 
\implies \H[\sigma] & = \log 2\sigma + 1
\end{align*}
\end{example}


\begin{example}
Let us apply \eqref{eqn:entropy_of_an_exponential_family} to compute the entropy of a univariate Gaussian.  The necessary exponential family quantities can be found in many tables (e.g. see \cite{nielsen2009statistical}); they are:
\[ \eta=\bigg(\frac{\mu}{\sigma^2}, \; -\frac{1}{2\sigma^2}\bigg)^T, \quad a(\eta) = \frac{-\eta_1^2}{4 \eta_2} + \half \log (-\frac{\pi}{\eta_2}), \quad  \nabla a(\eta) = \bigg( -\frac{\eta_1}{2\eta_2}, \; -\frac{1}{2\eta_2} + \frac{\eta_1^2}{4\eta_2^2}\bigg)^T\] 
and $h(x)=1$ (which allows us to use \eqref{eqn:entropy_of_an_exponential_family_with_base_measure_unity} instead of \eqref{eqn:entropy_of_an_exponential_family}).

Using this, we can easily compute 
\begin{align*}
\H[\eta]&= a(\eta) - \bigip{\eta, \nabla a(\eta)} \\
&=  \cancel{\frac{-\eta_1^2}{4 \eta_2}} + \half \log (-\frac{\pi}{\eta_2}) - \Big[ \cancel{\frac{-\eta_1^2}{2 \eta_2}} -\half + \cancel{\frac{\eta_1^2}{4 \eta_2}} \Big] \\ 
\implies \H[\sigma] & = \half \log (2 \pi \sigma^2) + \half
\end{align*}

Consider how much easier this computation is than that of, say, \url{https://proofwiki.org/wiki/Differential_Entropy_of_Gaussian_Distribution}.
\end{example}

\begin{remark}{\remarktitle{Cross-entropy between two distributions which belong to different exponential families}}

What does \eqref{eqn:cross_entropy_between_two_exponential_family_members} look like in the case that we we want to compute the cross-entropy between two distributions which belong to \textit{different} exponential families (with the same support)?

Let 
\begin{align*}
p(x \cond \eta) &=  \exp \{ \eta^T t(x) - a(\eta) + \log h(x)\}  \\
\wt{p}(x \cond \wt{\eta}) &=  \exp \{ \wt{\eta}^T \wt{t}(x) - \wt{a}(\wt{\eta}) + \log \wt{h}(x)\}  \\	
\end{align*}
 
Then the cross entropy from $\wt{p}$ to $p$ is given by
\begin{align*}
\H(p, \wt{p}) &= -\E_p [\log \wt{p}(x)]	\\
&= -\ds\int p(x \cond \eta) \; \log \wt{p} (x \cond \wt{\eta}) \; dx \\
&= -\ds\int p(x \cond \eta) \bigg[ \bigip{\wt{t}(x), \wt{\eta}} - \wt{a} (\wt{\eta}) + \log \wt{h}(x)  \bigg] dx \\
&= \wt{a}(\wt{\eta})- \bigip{ \E_p[\wt{t}(x)], \wt{\eta}} - \E_p[\log \wt{h}(x)]
\end{align*}

So there are now two potentially problematic terms, although in the case where $\wt{h}(x) \equiv 1$, only one potentially problematic term remains.
\end{remark}


For more information on information theoretical quantities in exponential families, including connection to Bregman divergences,  see \cite{nielsen2010entropies} or \cite{nielsen2009statistical}.


\subsubsection{Identifiability}

A model is called \textit{globally identifiable} if $p(\cdot \cond \theta_1) = p(\cdot \cond \theta_2)$ implies $\theta_1=\theta_2$ \cite{cole2020parameter}. For exponential families as defined in Def.~\ref{def:exponential_family},\footnote{Note that the log normalizer function is assumed to be continuously differentiable by definition.} if the Fisher information matrix is non-singular, then the model is globally identifiable.  For exponential families, we may compute the Fisher information matrix as\footnote{For justification, see \url{https://www2.stat.duke.edu/courses/Spring05/sta215/lec/wk06a.pdf.}}
\begin{align*}
I(\theta) = \E [ -\nabla^2 \log p(x \cond \theta)]
\labelit \label{eqn:fisher_information_matrix_for_exponential_families}	
\end{align*}
For more information on identifiability in exponential families, see \cite{cole2020parameter} pp.62. 

\subsubsection{i.i.d samples from an exponential family} \label{sec:iid_samples_from_an_exponential_family}
If $\+x=(x_1, ..., x_n)$ are n independent samples from the same exponential family, then 
\begin{align}
\label{eqn:exponential_family_iid}
 p(\+x \cond \theta) = \ds\prod_{i=1}^n h(x_i) \exp \big\{ \eta(\theta)^T \sum_{i=1}^n t(x_i) - n \,a(\eta(\theta))\big\} 
 \end{align}



\section{Frequentist Inference}
\subsection{Maximum Likelihood Estimation} \label{sec:ml_with_ef}

The goal for maximum likelihood is to determine the parameter
\begin{equation}
\label{eqn:ml}
\theta_{ML} = \argmax_\theta  \, \log p(\+x \cond \theta) 
\end{equation}

Let us assume that $\+x=(x_1, ..., x_n)$ are i.i.d observations  from a fixed exponential family, so that the likelihood has form \eqref{eqn:exponential_family_iid}.  Let us compute the gradient with respect to the natural parameter $\eta$ of $\ell(\eta) := \log p(\+x \cond \eta)$

\[ \nabla_\eta \ell(\eta) = \ds\sum_{i=1}^n t(x_i) - n \; \nabla_\eta a(\eta) \]

Setting the gradient to zero, we obtain

\[ \nabla_\eta a(\eta) = \df{1}{n}  \ds\sum_{i=1}^n t(x_i) \]

But $\nabla_\eta a(\eta) = \E [t(X)]$  (see Proposition \ref{prop:expected_sufficient_statistic_as_derivative_of_log_normalizer}).  Thus, we should set $\theta_{ML}$ such that

\[ \mu(\theta_{ML}) = \df{1}{n} \ds\sum_{i=1}^n t(x_i) \]
where $\mu := \E[t(X)]$ refers to the mean parametrization of the likelihood. \redfootnote{TODO: This switching of parameterization should be handled much more explicitly.}

\subsection{Expectation Maximization} \label{sec:em_with_ef}

Some models have latent variables associated with each observation, and so maximum likelihood is not possible.  Let us see how expectation maximization looks when the complete data likelihood is an exponential family.

The expectation maximization algorithm is 
\begin{equation}
\label{eqn:em_algorithm_appendix}
 \+\theta^{(t+1)} =  \text{argmax}_{\+\theta} \; \E_{p(\+z \cond \+x , \+\theta^{(t)})} \bigg[ \ln p(\+x, \+z \cond \+\theta) \bigg] 
 \end{equation}

We see how this plays out in exponential families by following the logic of Section \ref{sec:ml_with_ef}.   Let us assume that $(\+x, \+z)=((x_1,z_1), ..., (x_n, z_n))$ are n independent samples from the same exponential family, where $\+x$ is observed data and $\+z$ is unobserved data.
Moreover, let us assume that the complete data likelihood is an exponential family

\begin{align}
\label{eqn:exponential_family_complete_data_likelihood}
 p(\+x, \+z \cond \theta) = \ds\prod_{i=1}^n h(x_i, z_i) \exp \big\{ \eta(\theta)^T \sum_{i=1}^n t(x_i, z_i) - n \,a(\eta(\theta))\big\} 
 \end{align}

Here we want to find $\+\theta$ to optimize 
\[ f(\+\theta) =  \E_{p(\+z \cond \+x , \+\theta^{(t)})} \bigg[ \ln p(\+x, \+z \cond \+\theta) \bigg] \]

Following the logic of Section \ref{sec:ml_with_ef}, we determine that we should select $\theta^{(t+1)}$ such that
\[ \mu(\theta^{(t+1)}) = \df{1}{n} \ds\sum_{i=1}^n   \E_{p(\+z \cond \+x , \+\theta^{(t)})} t(x_i, z_i) \]
where $\mu := \E[t(x_1,z_1)]$ refers to the mean parametrization of the likelihood.

This is why an EM iteration is often described and/or implemented as performing maximum likelihood with the expected sufficient statistics.

\red{TODO: But is EM \textit{always} equivalent to performing ML with ESS's? Or is this ONLY true if I'm working within the exponential family?  I need to read up some more on EM theory.}

\red{TODO: Check this section, especially with respect to the fact that I am dealing with three parametrizations here - $\mu, \theta, \nu$; that is, mean, arbitrary, and natural, respectively.  Really the core problem is that it's not sufficiently clear in how head how and when reparametrizations affect things.}
 
 \section{Conjugate and semi-conjugate models} \label{sec:conjugate_exponential_family_models}

\input{conjugacy_and_semi_conjugacy}

\begin{remark}{\remarktitle{On conditional conjugacy}}
In other words,  a family of prior distributions for a parameter is called conditionally conjugate if the conditional posterior distribution (often called the \textit{complete conditional}),  given the data and all other parameters in the model,  is also in that class  \cite{gelman2006prior}. \redfootnote{Add some notes,  or refer back to notes from regular conjugacy (once they're created),  pointing out how this definition can be vapid,  and also how conjugate priors are not unique. }    In Section \ref{sec:normal_data_with_non_conjugate_prior}, we give perhaps the simplest example of a conditionally conjugate model.
 
\end{remark}
 
 Why are conjugate and conditionally conjugate models of interest?  The posterior distributions for conditionally conjugate models are easily approximated with Gibbs sampling or Mean Field Variational Inference -- the former samples from the complete conditional,  whereas the latter takes variational expectations with respect to the natural parameter of the complete conditional.   
 
 \begin{remark}
 Although most distributions with conjugate priors are exponential families, EF membership is not a \textit{necessary} condition for admitting a conjugate prior.  For instance, the uniform distribution on $[0,a]$ is not an exponential family (the distributions don't all have the same support), but the Pareto distribution is a conjugate prior for the parameter $a$ \cite{minka2001bayesian}.  
 \end{remark}
 	

 \subsection{Univariate normal model}
 
\subsubsection{Example:  Normal prior on mean of univariate Gaussian with known covariance}

\red{TODO: Fill in.  Note also that we can obtain this as a special case of the multivariate case, which is handled in Section \ref{sec:normal_prior_on_mvn_with_known_covariance}.}

\subsubsection{Example:  Inverse gamma prior on the variance of a univariate Gaussian with known mean}

\begin{proposition} \label{prop:bayes_univariate_normal_with_known_mean}
Consider the following Bayesian univariate normal model with known mean $\mu$ and random variance $\sigma^2$
\begin{align*}
\sigma^2 &\sim \IG (\alpha_0,  \beta_0) \\
y_i \cond \mu,  \sigma^2 &\iid \N (\mu,  \sigma^2),  \quad i=1,...,n \\
\labelit \label{eqn:bayesian_uvn_known_mean}
\intertext{where $\IG$ denotes the Inverse Gamma distribution.  The posterior distribution is given by}
\sigma^2 \cond \+y,  \mu & \sim \IG \bigg(\alpha_0 + \frac{n}{2}\ , \; \beta_0 + \half  \ds\sum_{i=1}^n (x_i - \mu)^2  \bigg) \labelit \label{eqn:posterior_on_uvn_variance_under_standard_parameterization}
\end{align*}

\end{proposition}

\begin{proof}

We have 
\begin{align*}
p(\sigma^2 \cond \+y,  \mu) &\stackrel{1}{\propto}  \explaintermbrace{prior}{ p(\sigma^2)} \quad \explaintermbrace{likelihood}{ \ds\prod_{i=1}^n p(y_i \cond \mu,  \sigma^2)} \\
& \stackrel{2}{\propto} \explaintermbrace{prior}{(\sigma^2)^{-\alpha_0 - 1} \exp \big\{ -\df{\beta_0}{\sigma^2} \big\}} \quad  \explaintermbrace{likelihood}{(\sigma^2)^{-n/2}  \,  \exp \big\{ -\df{1}{2\sigma^2}  \ds\sum_{i=1}^n (y_i - \mu)^2 \big\}}  \\
& \stackrel{3}{\propto} (\sigma^2)^{- (\alpha_0 + n/2) - 1} \exp \bigg\{ -\df{\beta_0 + \half \sum_{i=1}^n (y_i - \mu)^2}{\sigma^2} \bigg\}
\end{align*}
where (1) is by Bayes rule (and conditional independence of the observation model),  (2) fills in the pdfs,  and (3) combines like terms so as to look like an Inverse Gamma density.
\end{proof}

\begin{remark}{\remarktitle{Reparametrizing the inverse gamma prior for greater interpretability}}  
\label{rk:inverse_gamma_prior_with_hoff_parametrization}
As observed by Peter Hoff \cite{hoff2009first} (pp.74), the form of \eqref{eqn:posterior_on_uvn_variance_under_standard_parameterization}  suggests parametrizing the prior as 
\begin{align*}
\sigma^2 &\sim \IG (\frac{\nu_0}{2},  \,   \frac{\nu_0 \, \sigma^2_0}{2})
\intertext{for greater intepretability.   In this case,  we find that the posterior is given by}
\sigma^2 \cond \+y,  \mu &\sim \IG \bigg( \frac{\nu_0 + n }{2} , \quad  \frac{ \nu_0 \, \sigma_0^2 + n \, \widehat{\sigma}^2_{\text{MLE}}}{2} \bigg) 
%\alpha_n & =  \frac{\nu_0 + n }{2}\\
%\beta_n & =  \frac{ \nu_0 \, \sigma_0^2 + n \, \widehat{\sigma}^2_{\text{MLE}}}{2}
\intertext{where the maximum likelihood estimator of the variance $\widehat{\sigma}^2_{\text{MLE}}$ is defined by}
\widehat{\sigma}^2_{\text{MLE}} & :=  \df{1}{n} \ds\sum_{i=1}^n (y_i - \mu)^2 
\end{align*}
So $\nu_0$ plays the role of a prior sample size and $\sigma^2_0$ plays the role of the variance within that prior sample.\footnote{Note that the maximum likelihood estimator of the variance, $\widehat{\sigma}^2_{\text{MLE}}$, could also be expressed as the mean squared error, $\text{MSE}$.}.  In other words, this reparametization gives us an interpretation of the the prior in terms of ``equivalent data" \cite{box2011bayesian}.\footnote{I like this description. I first saw it referenced on pp. 517 of \cite{gelman2006prior}.}
% \eqref{eqn:bayesian_uvn_known_mean}
 
\end{remark}

 
 
\subsection{Multivariate normal model}
 
\subsubsection{Example:  Normal prior on mean of multivariate Gaussian with known covariance}  \label{sec:normal_prior_on_mvn_with_known_covariance}
 
 
Here we provide the posterior for the mean of a multivariate Gaussian in the case where the covariance is known.

Given data $\+y :=(\+y_1, ....,\+y_n)$, consider the model
\begin{align*}
\+\mu &\sim \N(\+\mu_0, \+\Sigma_0) \\
\+y_i \cond \+\mu &\iid \N(\+\mu, \+\Sigma) && i=1,...,n
\end{align*}




We use the exponential family representation of the MVN (Example \ref{ex:mvn_as_ef}) to represent the prior in terms of its natural parameters
\begin{align*}
	p(\+\mu) &\propto \exp \bigg\{ -\half \+\mu^T \+\Sigma_0^{-1} \+\mu + \+\mu^T \+\Sigma_0^{-1} \+\mu_0 \bigg\} 
\labelit \label{eqn:prior_for_mvn_fixed_cov}
\end{align*}

And similarly, we write the likelihood $	L(\+\mu) = p(\+y \cond \+\mu) = \prod_{i=1}^n p(\+y_i \cond \+\mu)$  as
\begin{align*}
	L(\+\mu) &\propto \exp \bigg\{ -\half \ds\sum_{i=1}^n (\+y_i - \+\mu)^T \+\Sigma^{-1} (\+y_i - \+\mu)\bigg\}  \\
	&= \exp \bigg\{ - \half \+\mu^T n \+\Sigma^{-1} \+\mu + \+\mu^T \+\Sigma^{-1} n \bar{\+y} \bigg\} 
\labelit \label{eqn:likelihood_for_mvn_fixed_cov}
\end{align*}



So by Bayes' law, combining the like terms in $\+\mu$ of \eqref{eqn:prior_for_mvn_fixed_cov} and \eqref{eqn:likelihood_for_mvn_fixed_cov}, we find
\begin{align*}
 p(\+\mu \cond \+y) &\propto \explaintermbrace{prior}{p(\+\mu)}\explaintermbrace{likelihood}{p(\+y \cond \+\mu)} \\
 &= \exp \bigg\{ - \half \+\mu^T \bigg( \+\Sigma_0^{-1} + n  \+\Sigma^{-1}\bigg)  \+\mu + \+\mu^T \bigg( \+\Sigma_0^{-1} \+\mu_0 + n \+\Sigma^{-1} \bar{\+y} \bigg) \bigg\} 
\end{align*}
which reveals that the posterior is normal (Remark \ref{rk:mvn_from_ef}), along with the particular forms for its natural parameters (precision and precision-weighted mean).  In particular, we have, $\+\mu \cond \+y \sim \N(\+\mu_n, \+\Sigma_n)$, where
\begin{align*}
	\+\Sigma_n &= \bigg( \+\Sigma_0^{-1} + n \+\Sigma^{-1}\bigg)^{-1} \\
	\+\mu_n &= \+\Sigma_n \bigg(\+\Sigma_0^{-1} \+\mu_0 + n \+\Sigma^{-1} \bar{\+y}  \bigg)
\end{align*}

On the precision scale,  $\+\Sigma_n$ is the sum of the prior precision matrix $\+\Sigma_0^{-1}$ and $n$ copies of the precision for each observation,  $\+\Sigma^{-1}$.    Similarly,  $\+\mu_n$ is the precision-weighted convex combination of $\+\mu_0$, the prior mean, and the empirical average, $\bar{\+y}$.

\subsubsection{Example:  Inverse Wishart prior on covariance matrix of multivariate Gaussian with known mean} \label{sec:inverse_wishart_prior_on_mvn_with_known_mean}
 
Here we will show that the Inverse Wishart is a conjugate prior for the covariance of a multivariate normally distributed random variable with known mean.

This situation comes up 

\begin{example}{\remarktitle{Inverse Wishart prior on the covariance of a  Multivariate Normal sampling model with known mean}} \label{ex:inverse_wishart_prior}



Consider the sampling model for $\+y :=(\+y_1, ....,\+y_n) \iid \N_d(\+\mu,  \+\Sigma)$
\begin{align}
p(\+y \cond \+\mu, \+\Sigma) & \propto | \+\Sigma |^{-n/2} \exp \bb{ -\df{1}{2} \ds\sum_{i=1}^n (\+y_i - \+\mu)^T  \Sigma^{-1} (\+y_i- \+\mu) } \nonumber \\
& = | \+\Sigma |^{-n/2} \exp \bb{ -\df{1}{2} \tr(\+\Sigma^{-1} \+S_\mu)  }\label{eqn:mvn_in_nice_form_for_inverse_wishart_prior}
\end{align}
where $\+S_\mu := \sum_{i=1}^n (\+y_i - \+\mu) (\+y_i - \+\mu)^T$ is the sum of pairwise deviation products,  and where the equality in \eqref{eqn:mvn_in_nice_form_for_inverse_wishart_prior}  is justified in Remark \ref{rk:reexpressing_mvn_in_nice_form_for_inverse_wishart_prior}.

Let us take the mean $\+\mu$ to be known,  and let us take the prior on the covariance $\+\Sigma \in \R^{d \times d}$ to be given by $\+\Sigma \sim \InvWish(\+\Psi, \nu)$, i.e.

\begin{align}
p(\+\Sigma) \propto | \+\Sigma|^{-(\nu+d +1) /2}  \exp \bb{ -\df{1}{2} \tr ( \+\Sigma^{-1} \+\Psi) } 
\label{eqn:inverse_wishart_prior}
\end{align}
where $\+\Sigma \succ 0$ and $\nu > d-1$ to have a proper prior.   Note that $\E[\+\Sigma] = \frac{\+\Psi}{\nu -d -1}$.

It is easy to see from the forms of the likelihood \eqref{eqn:mvn_in_nice_form_for_inverse_wishart_prior} and prior \eqref{eqn:inverse_wishart_prior} that the Inverse Wishart is a conjugate prior in this context.  In particular

\begin{align}
p(\+\Sigma \cond \+\mu,  \+y) \propto | \+\Sigma|^{-(\nu+ n + d +1) /2}  \exp \bb{ -\df{1}{2} \tr \big(\+\Sigma^{-1} (\+\Psi + \+S_\mu) \big) } 
\label{eqn:inverse_wishart_posterior}
\end{align}
where $S_\mu$ was defined above.  Thus,  we have 
\[ \+\Sigma \cond \+\mu,  \+y \sim \InvWish \bigg( \+\Psi +  \ds\sum_{i=1}^n (\+y_i - \+\mu) (\+y_i - \+\mu)^T,  \nu + n \bigg) \]
And so the conjugate updates are given by
\begin{align}
\nu^\prime &\leftarrow  \nu + n \\
\+\Psi^\prime &\leftarrow \+\Psi + \ds\sum_{i=1}^n (\+y_i - \+\mu) (\+y_i - \+\mu)^T
\end{align}
\end{example}

For interpretation of the parameters of the Inverse Wishart,  see Remark \ref{rk:inverse_wishart_parameter_interpretation}.

\begin{remark}{\remarktitle{Expressing the Multivariate Gaussian density in a nice form for the Inverse Wishart prior on the Covariance Matrix}} 
\label{rk:reexpressing_mvn_in_nice_form_for_inverse_wishart_prior}

Here we justify the equality of \eqref{eqn:mvn_in_nice_form_for_inverse_wishart_prior}.

We will show that $\ds\sum_{i=1}^n \+x_i^T \+A \+x_i = \tr(\+A \ds\sum \+x_i \+x_i^T)$ for $\+x \in \R^d$,  and $\+A \in \R^{d \times d}$ symmetric.

\begin{align*}
\ds\sum_{i=1}^n \+x_i^T \+A \+x_i  &= \ds\sum_{i=1}^n \ds\sum_{j,k=1}^n a_{jk} x_{ij} x_{ik} \\
& = \ds\sum_{j,k=1}^n \bigg( \+A \circ \ds\sum_{i=1}^n \+x_i \+x_i^T \bigg)_{jk} \\
& \stackrel{(*)}{=} \tr (\+A \ds\sum_{i=1}^n \+x_i \+x_i^T) 
\end{align*}
where $\circ$ is the Hadamard,  also called the elementwise,  operator,  and where (*) holds by properties of the $\tr$ operator

\[ \tr(\+A\+B) = \ds\sum_{i,j} (\+A^T \circ \+B)_{ij}  \stackrel{\text{$\+A$ symmetric}}{=}  \ds\sum_{i,j} (\+A \circ \+B)_{ij}\]

\end{remark}

\subsubsection{Example:  Bayesian normal model with conditionally conjugate prior} \label{sec:normal_data_with_non_conjugate_prior}

Consider the following model with a normal sampling distribution and conditionally conjugate prior\redfootnote{TODO: Prove that the prior,  although conditionally conjugate,  is not conjugate.   (I \textit{believe} this is true,  based on context clues from experience,  but I am not currently certain about it.)}:
\begin{align*}
\+\mu &\sim \N_{d}(\+m_0,\+V_0 ) \\
\+\Sigma &\sim \InverseWishart(\nu_0,  \+\Psi_0) \\
\+x_i \cond \+\mu,  \+\Sigma &\iid \N_{d}( \+\mu , \+\Sigma), \quad i=1,...,N
\end{align*}
We define $\+x := (\+x_1,  \hdots \+x_N)$,  where each $\+x_i \in \R^d$.

The complete conditionals are well-known,  and have in fact already been provided by Sections  \ref{sec:normal_prior_on_mvn_with_known_covariance} and \ref{sec:inverse_wishart_prior_on_mvn_with_known_mean}. \footnote{We still need to add a derivation for \eqref{eqn:normal_model_complete_conditional_on_mu} \red{TODO},  but the birds' eye view for one approach is to use the general formalism for conjugacy updates in the exponential family \eqref{eqn:general_formalism_prior_to_posterior_conversion},  noting that the natural parameters for a multivariate Gaussian are its precision and precision-weighted mean. }    In particular
\begin{align*}
\+\mu  \cond \+\Sigma, \+x &\sim \N_{d}(\+m,\+V )  \labelit\label{eqn:normal_model_complete_conditional_on_mu} \\
\intertext{where}
\+m  &=  \bp{\+V_0^{-1} + N  \+\Sigma^{-1} }^{-1}  \bp{\+V_0^{-1} \+m_0 + N \+\Sigma^{-1}  \bar{\+x} } \\
\+V &= \bp{\+V_0^{-1} +  N \+\Sigma^{-1} }^{-1} \\
\intertext{and}
\+\Sigma \cond \+\mu,  \+x  &\sim \InverseWishart(\nu,  \+\Psi) 
\labelit \label{eqn:normal_model_complete_conditional_on_Sigma} \\
\intertext{where}
\nu &=  \nu_0 + N \\
\+\Psi &= \Psi_0 + \ds\sum_{i=1}^N  (\+x_i - \+\mu) (\+x_i - \+\mu)^T 
\labelit \label{eqn:Sigma_cc_normal_model_cond_conj_prior}\\ 
\end{align*}

Note that the model is different than the model  fully conjugate (Normal-Inverse-Wishart) prior on the pair $(\+\mu, \+\Sigma)$.   The conditionally conjugate prior lacks closed-form posterior updating,  but is also more expressive. \bluefootnote{Is it also more expressive once we move to a variational approximation?  i.e.,  can we get more expressive marginals this way?}

These conjugate posterior updates have nice interpretations:
\begin{itemize}
\item \textbf{Hyperparameter updates for $(\+\mu  \cond \+\Sigma, \+x)$}: On the precision scale,  $\+V$ is the sum of the prior precision matrix $\+V_0^{-1}$ and $N$ copies of the precision for each observation,  $\+\Sigma^{-1}$.    Similarly,  $\+m$ is the precision-weighted convex combination of $\+m_0$, the prior mean,    and the empirical average, $\bar{\+x}$.
\item \textbf{Hyperparameter updates for $(\+\Sigma \cond \+\mu,  \+x)$}:  The covariance was estimated from $\nu$ observations with a sum of pairwise deviation products $\Psi$.
\end{itemize}

\begin{remark}{\remarktitle{Purpose of a prior on $\+\Sigma$}} \label{rk:purpose_of_prior_on_cov_matrix}
As mentioned by \cite{imai2005bayesian} (pp. 315) a prior distribution on $\+\Sigma$ is generally not meant to convey substantive information, but rather to be weakly informative, and provide some shrinkage of the eigenvalues (i.e., the variances along the principal directions) and correlations. 	
\end{remark}


\subsection{Bayesian linear regression}

\subsubsection{Example:  Bayesian linear regression with normal prior on regression weights and known observation noise} \label{sec:Bayesian_linear_regression_with_normal_prior}


In this section,  we will show that the normal prior on $\+\beta$ is a conjugate prior for the regression weights $\+\beta$ of a Bayesian multiple regression model with known observation noise $\sigma^2$.  That is,  the posterior on $\+\beta$ given $\+y = (y_1,...,y_n)^T$ for such a model is also Gaussian.  


\begin{proposition} \label{prop:bayes_linear_regression_with_known_ssq}
Consider the Bayesian linear multiple regression model with known observation noise $\sigma^2$
\begin{align*}
\+\beta &\sim \N (\+\mu_0, \+\Sigma_0) \\
y_i \cond \+\beta,  \sigma^2  &\indsim \N (\+x_i^T \+\beta, \sigma^2),  \quad i=1,...,n \\
\labelit \label{eqn:bayesian_multiple_linear_regression_with_known_observation_noise}
\end{align*}
where  $\+x_i$ designates the $i$-th row of the design matrix $\+X \in \R^{n \times p}$.

The posterior distribution for \eqref{eqn:bayesian_multiple_linear_regression_with_known_observation_noise} is given by 
\begin{align*}
\+\beta \cond \+y,  \sigma^2 & \sim \N(\+\mu,  \+\Sigma )
\intertext{where}
\+\Sigma &= \bp{\+\Sigma_0^{-1} +  \frac{1}{\sigma^2} \+X^T \+X }^{-1}  \\
\+\mu &= \+\Sigma \bp{   \+\Sigma_0^{-1} \+\mu_0 +  \frac{1}{\sigma^2} \+X^T  \+y}  \\
\labelit \label{eqn:posterior_bayesian_lin_regression_with_known_obs_var}
\end{align*}

\end{proposition}


\begin{proof}

First,  we consider the likelihood $L(\+\beta) := p(\+y \cond \+\beta,  \sigma^2$),  dropping terms proportional to  $\+\beta$.
\begin{subequations}
\begin{align}
p(\+y \cond \+\beta,  \sigma^2) & \propto  \exp \bigg\{  -\df{1}{2 \sigma^2}  (\+y - \+X \+\beta)^T (\+y - \+X \+\beta) \bigg\} \label{eqn:bayesian_lin_regression_likelihood_direct_expression}\\ 
&=   \exp \bigg\{  -\df{1}{2 \sigma^2}  (- 2 \+\beta^T \+X^T \+y + \+\beta^T \+X^T \+X \+\beta) \bigg\} 
\end{align}
\label{eqn:bayesian_lin_regression_likelihood}
\end{subequations}
Doing the same for the prior $p(\+\beta)$,  we have 
\begin{align}
p(\+\beta) & \propto  \exp \bigg\{  -\df{1}{2}  (- 2 \+\beta^T \+\Sigma_0 \+\mu_0 + \+\beta^T \+\Sigma_0^{-1} \+\beta) \bigg\}
\label{eqn:bayesian_lin_regression_prior}
\end{align}

Thus,  by Bayes rule
\begin{align*}
p(\+\beta \cond  \+y ,  \sigma^2 ) & \propto  p(\+y \cond \+\beta,  \sigma^2) \times p(\+\beta)  \\
& \propto  \exp \bigg\{   \+\beta^T \explaintermbrace{$:=\+b$}{\bigg(\+\Sigma_0^{-1} \+\mu_0 +  \frac{1}{\sigma^2}\+X^T \+y \bigg)} - \half \+\beta^T   \explaintermbrace{$:=\+A$}{\bigg(\+\Sigma_0^{-1}  +  \frac{1}{\sigma^2} \+X^T \+X \bigg)}  \+\beta \bigg\}
\end{align*}

which reveals that the posterior is normal (Remark \ref{rk:mvn_from_ef}),  along with the particular form of its parameter  (covariance $\+A^{-1}$ and mean $\+A^{-1} \+b$). 


\end{proof}


\begin{remark}
For a nice conceptual overview of Bayesian linear regression, see \cite{grosee2019bayesian} or \cite{bishop2006pattern}.   Among other things, these resources demonstrate how Bayesian regression makes predictions using an infinite collection of regression models (whose contributions are weighted by their posterior probabilities).  They also show how the linear model is less restrictive than it might first seem;  it can be used to model nonlinear functional relationships by using nonlinear basis functions. 
\end{remark}

\begin{remark}{\remarktitle{Posterior parameters in terms of maximum likelihood estimates}} Now recall
\begin{align*}
\betaML &= (\+X^T \+X)^{-1} \+X^T \+y \\
\Var(\betaML) &= \sigma^2 (\+X^T \+X)^{-1}  
\end{align*}

So the posterior parameters in \eqref{eqn:posterior_bayesian_lin_regression_with_known_obs_var} have the interpretation 
\begin{align*}
\explaintermbrace{posterior precision}{\wt{\+\Sigma}^{-1}} &= \explaintermbrace{prior precision}{\+\Sigma_0^{-1}} +  \explaintermbrace{``data" precision,  \; $\Var(\betaML)^{-1}$}{\frac{1}{\sigma^2} \+X^T \+X}   \\
\explaintermbrace{posterior precision-weighted mean}{\wt{\+\Sigma}^{-1} \wt{\+\mu}} &= \explaintermbrace{prior precision-weighted mean}{\+\Sigma_0^{-1} \+\mu_0} +  \explaintermbrace{``data" pwm\; $\Var(\betaML)^{-1} \betaML$}{\frac{1}{\sigma^2} \+X^T  \+y}
\end{align*}
\label{rk:relation_of_bayesian_posterior_parameters_to_maximum_likelihood_estimate}
\end{remark}

% NOTE TO SELF: The following provides more mechanistic insight into the relationship between the Bayesian posterior and the maximum likelihood estimates, but there were a few holes, and I wasn't able to address them before getting pulled into an annoying meeting.  So I am just commenting them out in favor of Remark~\ref{rk:relation_of_bayesian_posterior_parameters_to_maximum_likelihood_estimate}, which provides the same take-home message but without the mechanistic insight. 
%
%\begin{remark}
%By the form of the exponential family density \eqref{eqn:exponential_family_natural}, we immediately see that the product of two exponential family densities with identical sufficient statistics but different natural parameters is obtained by simply summing the natural parameters.  So if we see the likelihood \eqref{eqn:bayesian_lin_regression_likelihood} and the prior \eqref{eqn:bayesian_lin_regression_prior} both as functions of $\+\beta$, we notice that the posterior parameters in \eqref{eqn:posterior_bayesian_lin_regression_with_known_obs_var} have the interpretation 
%%
%\begin{align*}
%\explaintermbrace{posterior precision}{\+\Sigma^{-1}} &= \explaintermbrace{prior precision}{\+\Sigma_0^{-1}} +  \explaintermbrace{data precision}{\frac{1}{\sigma^2} \+X^T \+X}   \\
% \explaintermbrace{posterior precision-weighted mean}{\+\Sigma^{-1} \+\mu} &=   \explaintermbrace{prior precision-weighted mean}{\+\Sigma_0^{-1} \+\mu_0} +   \explaintermbrace{data precision-weighted mean}{\frac{1}{\sigma^2} \+X^T  \+y}  
%\end{align*}
%where the data precision can be obtained by refactoring the likelihood \eqref{eqn:bayesian_lin_regression_likelihood_direct_expression} so that $\frac{1}{\sigma^2} \+X^T \+X$ is the matrix in the quadratic form  and where the ``data mean" is the maximum likelihood estimator $(\+X^T\+X)^{-1} \+X^T \+y$. 
%\end{remark}

\begin{remark}{\remarktitle{Bayesian  linear regression as a compromise between the prior and maximum likelihood value.}}
Equation \eqref{eqn:posterior_bayesian_lin_regression_with_known_obs_var}
gives the posterior for Bayesian linear multiple regression in the case where the observation noise is known.    As pointed out by \cite{hoff2009first} (pp. 155),  intuition can be obtained by considering the limiting cases.  When the prior on the regression coefficients $\+\beta$ is diffuse,  the elements of the prior precision matrix $\+\Sigma_0^{-1}$ will be small,  and so the posterior mean satisfies
$\+\mu \approx ( \+X^T \+X)^{-1} \+X^T \+y$,  i.e.  it approximately equals the standard least squares estimate.   On the other hand,   when the observation variance $\sigma^2$ is large,  then the measurement precision is small, and the posterior mean satisfies $\+\mu \approx \+\mu_0$,  i.e.  it approximately equals the prior mean.
\end{remark}

\paragraph{Posterior predictive}

The posterior predictive distribution for Bayesian linear regression with known observation noise \eqref{eqn:bayesian_multiple_linear_regression_with_known_observation_noise},   after observing n observations $\+y =(y_1, ..., y_n)$,   has density
\begin{align*}
p(y_\new \cond \+y) &= \ds\int  p(y_\new \cond \+\beta) \, p(\+\beta \cond \+y) \, d\+\beta \\
&= \ds\int f_\N(\+x_\new^T \+\beta,  \sigma^2) \,  f_\N(\+\mu_n,  \+\Sigma_n)  \,  d\+\beta \\
& \stackrel{1}{=} f_\N \bigg( \+x_\new^T \+\mu_n,  \;  \sigma^2 + \+x_\new^T \+\Sigma_n  \+x_\new \bigg) 
\end{align*}

where $f_\N(m,v)$ refers to the density of a univariate Gaussian with mean $m$ and variance $v$, and where $(\+\mu_n, \+\Sigma_n)$ are the posterior parameters given by \eqref{eqn:posterior_bayesian_lin_regression_with_known_obs_var},  and where Equality (1) holds by Proposition \ref{prop:simplest_linear_gaussian_model}.


\subsubsection{Example:  Bayesian linear regression with inverse gamma prior on observation noise and known regression weights} \label{sec:Bayesian_linear_regression_with_IG_prior}


In this section,  we will show that the Inverse Gamma prior on $\sigma^2$ is a conjugate prior for the observation noise of a Bayesian multiple regression model with known regression weights  $\+\beta$.  That is,  the posterior on $\sigma^2$ given $\+y = (y_1,...,y_n)^T$ for such a model is also Inverse Gamma. 


\begin{proposition} \label{prop:bayes_linear_regression_with_known_weights}
Consider the Bayesian linear multiple regression model with known regression weights  $\+\beta$ 
\begin{subequations}
\begin{align}
\sigma^2 &\sim \InverseGamma \bigg(\frac{\nu_0}{2},  \frac{\nu_0 \sigma^2_0}{2} \bigg) \\
\+y \cond \+\beta,  \sigma^2  &\sim \N (\+X \+\beta, \sigma^2 \+I_n) 
\end{align}
\label{eqn:bayesian_multiple_linear_regression_with_known_weights}
\end{subequations}

where  $\+x_i$ designates the $i$-th row of the design matrix $\+X \in \R^{n \times p}$.


The posterior distribution for \eqref{eqn:bayesian_multiple_linear_regression_with_known_weights} is given by 
\begin{subequations}
\begin{align}
\sigma^2 \cond \+y,    \+\beta  & \sim \InverseGamma \bigg(\frac{\nu_0 + n }{2},  \frac{\nu_0 \sigma^2_0  + \text{SSR}(\+\beta)}{2} \bigg)  
\intertext{where the sum of squared residuals is}
\text{SSR}(\+\beta)  &= (\+y - \+X \+\beta)^T  (\+y - \+X \+\beta)
\end{align}
\label{eqn:posterior_bayesian_lin_regression_with_known_weights}
\end{subequations}

\end{proposition}

\begin{proof}\footnote{For corroboration, see \cite[pp.~155]{hoff2009first}, who obtains the same result.}

\begin{align*}
p(\sigma^2 \cond \text{rest}) & \propto \explaintermbrace{$\+y \cond \+\beta,  \sigma^2  \sim \N (\+X \+\beta, \sigma^2 \+I_n) $}{\bigg(\frac{1}{\sigma}\bigg)^n	 \exp \bigg( -\half \frac{1}{\sigma^2} (\+y - \+X \+\beta)^T (\+y - \+X \+\beta)}\bigg) \; \explaintermbrace{$\sigma^2 \sim \InverseGamma \big(\frac{\nu_0}{2},  \frac{\nu_0 \sigma^2_0}{2}\big)$}{\bigg( \sigma^2 \bigg )^{-\frac{\nu_0}{2} -1}  \exp \bigg( -\frac{\half \nu_0 \sigma_0^2}{\sigma^2} \bigg)} \\
& = (\sigma^2)^{-\frac{n}{2} - \frac{\nu_0}{2} - 1} \exp \bigg( -\frac{\half (\text{SSR}(\+\beta) + \nu_0 \sigma_0^2)}{\sigma^2}\bigg)
\end{align*}


\end{proof}
%\red{TODO: Add expression for the marginal likelihood,  which is available in closed form.  See e.g.  \cite{gunderson2020bayesian},  although that resource uses a NIG and not independent N and IG priors.}
 
\subsection{Hierarchical Bayesian linear regression} \label{sec:hierarchial_linear_regression}

Consider a Bayesian hierarchical linear regression.    We take the regression to be hierarchical in the sense that we take the regression weights $\+\beta_j$ to be distinct for each of $j=1,...,J$ groups,  but we assume that the $\+\beta_j$'s are drawn from some distribution.     The model allows for ``sharing statistical strength" in the sense that uncertainty about the $j$th group's regression parameters,  to the extent that it exists,  can be reduced by borrowing information from the other groups $k \neq j$.    In other words,  for grouped data,  we allow the information from the other groups to play the role that is played by the prior in Bayesian linear regression.To further motivate this model,  see \cite{hoff2009first}.

A simple version of this model is\footnote{The version is simple because,  for example,   we ignore problems with the Inverse Wishart for modeling covariance matrices (see Section \ref{sec:inverse_wishart_distribution}),  we are not imagining that the regression coefficients are sparse,  etc.} \footnote{Recall that the inverse gamma distribution is parametrized in a convenient way for interpretability,  where $\nu_0$ is a prior sample size from which a  prior sample variance of $\sigma_0^2$ has been obtained.  This parametrization,  and corresponding interpretation,  falls out of the use of the inverse gamma as a prior on the variance in a univariate normal model (see Remark \ref{rk:inverse_gamma_prior_with_hoff_parametrization}). }:
\begin{align*}
\+\mu &\sim  \N (\+m_0,\+V_0) \\
\+\Sigma &\sim \InvWish(\eta_0,  \+\Psi_0) \\
\+\beta_j &\iid \N(\+\mu,  \+\Sigma) \\
\sigma^2 &\sim \InverseGamma (\frac{\nu_0}{2}, \frac{\nu_0}{2} \sigma_0^2) \\
\epsilon_{ij} &\iid \N(0, \sigma^2) \\
y_{ij} &= \+\beta_j^T \+x_{ij} + \epsilon_{ij} \\
\labelit \label{eqn:hierarchical_linear_regression}
\end{align*}
This model can be seen as a Bayesian linear regression to model within-group data,  put beneath a Bayesian normal sampling model to handle between-group heterogeneity in the regression weights. 
 
The complete conditionals (e.g.  see Section 11.2 of \cite{hoff2009first}) are given by

\begin{align*}
\+\beta_j \cond \+\Sigma,  \+\mu, \sigma^2,  \+y & \sim  \N(\+\mu_j^\prime,  \+\Sigma_j^\prime) \\
\+\Sigma_j^\prime &= \bigg( \+\Sigma^{-1} + \frac{1}{\sigma^2} \+X_j^T \+X_j \bigg)^{-1} \\
\+\mu_j^\prime &= \+\Sigma_j^\prime \bigg( \+\Sigma^{-1} \+\mu + \frac{1}{\sigma^2} \+X_j^T \+y_j \bigg) \\
& \\ 
\sigma^2 \cond \+\beta_1, ...,\+\beta_J,  \+y & \sim \InverseGamma
\bigg( \half (\nu_0 + N),  \half (\nu_0 \sigma_0^2 + \text{SSR(\+\beta)})\bigg) \\
\intertext{where}
N &:= \sum_{j=1}^J n_j \\
\text{SSR(\+\beta)} &:= \ds\sum_{j=1}^J \ds\sum_{i=1}^{n_j}  (y_{ij} - \+\beta_j^T \+x_{ij})^2 \\
& \\ 
\+\mu \cond  \+\beta_1, ...,\+\beta_J,  \+\Sigma &\sim \N(\+m^\prime,  \+V^\prime) \\
\+m^\prime &= \+V^\prime \bigg( \+V_0^{-1} \+m_0 + J \+\Sigma^{-1} \overline{\+\beta} \bigg),  \quad \quad \overline{\+\beta} := \frac{1}{J} \sum_{j=1}^J \+\beta_j \\ 
\+V^\prime &=  \bigg( \+V_0^{-1} + J \+\Sigma^{-1} \bigg)^{-1} \\
& \\ 
\+\Sigma \cond \+\beta_1, ...,\+\beta_J,  \+\mu &\sim \InverseWishart (\eta^\prime,  \+\Psi^\prime) \\
\eta^\prime &= \eta_0 + J \\
\+\Psi^\prime &= \+\Psi_0 + \ds\sum_{j=1}^J (\+\beta_j - \+\mu) (\+\beta_j -\+\mu)^T \\
\labelit \label{eqn:hierarchical_linear_regression_ccs}
\end{align*}

Where note that we have defined,  as shorthands,  
\begin{align*}
\+\mu_j^\prime  & := \E [ \+\beta_j \cond \+\Sigma, \+\mu, \sigma^2, \+y] \\
\+\Sigma_j^\prime  & := \Var [ \+\beta_j \cond \+\Sigma, \+\mu, \sigma^2, \+y] \\
\end{align*}
and likewise throughout \eqref{eqn:hierarchical_linear_regression_ccs}. 

Following are some thoughts on these complete conditionals; in particular,  on their relationship to the complete conditionals from the simpler models (Bayesian linear regression,  multivariate normal sampling model) from which the Bayesian hierarchical linear regression is composed:
\begin{itemize}
\item The complete conditional  for the expected regression weights across groups,  $\+\mu$,   is just the conditional distribution for the mean of a multivariate normal sampling model \eqref{eqn:normal_model_complete_conditional_on_mu},  but where the ``data" are the (latent) regression weights,  $\+\beta_1, ...,\+\beta_J$. 
\item The complete conditional  for the variance in regression weights across groups,  $\+\Sigma$,   is just the conditional distribution for the variance of a multivariate normal sampling model \eqref{eqn:normal_model_complete_conditional_on_Sigma},  but where the ``data" are the (latent) regression weights, $\+\beta_1, ...,\+\beta_J$. 
\item The complete conditionals for the group-specific regression weights,  $\+\beta_j$,  are just the conditional distributions for the regression coefficients from Bayesian linear regression \eqref{eqn:bayesian_multiple_linear_regression_with_known_observation_noise},  but where we use group $j$'s data alone,  and where the prior on these regression weights is not an external prior,  but a normal distribution with mean equal to $\+\mu$,  the expected regression weights across groups,  and variance equal to $\+\Sigma$,  the variance in regression weights across groups. 
\end{itemize}

\begin{question}
The hierarchical linear regression model \eqref{eqn:hierarchical_linear_regression} parametrizes the prior on the variance $\sigma^2$ such that its hyperparameters $(\sigma_0^2,  \nu_0)$ can be interpreted as the sample variance and sample size of prior observations.   However,  there is a weird asymmetry because we don't construct the prior on the mean in this manner.   It would be nice to provide the option of doing so.  	My guess is that in the simplest form,  this would just mean parameterizing the top-level prior on $\+\mu$ to have a variance given by $\frac{1}{\kappa_0} V_0$, where $\kappa_0$ is the number of psuedo observations relevant to estimating the mean.    See pp.74-75 of \cite{hoff2009first} for ideas,  although that discussion takes the prior on the mean to depend on the sampling variance $\sigma^2$.  
\end{question}

\subsection{General formalism} \label{sec:ef_general_formalism}

\input{general_conjugacy_formalism}

\bibliography{references_exponential_family}
\bibliographystyle{apalike}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
 \appendix
 

\section{Matrix Facts}

\subsection{Multivariate completing the square}

A nice overview of multivariate completing the square is given by \cite{gundersen2019completing}.   See also \cite[pp.~86]{bishop2006pattern} for application to the Gaussian case.

Let $\+x,  \+b$ be $d$-dimensional vectors, and let $\+M \in \R^{d \times d}$ be a symmetric invertible matrix. Then
%
\begin{align*}
\+x^T \+M \+x - 2 \+b^T \+x = (\+x - \+M^{-1} \+b)^T \+M (\+x - \+M^{-1} \+b) - \+b^T \+M^{-1} \+b
 \labelit \label{eqn:multivariate_completing_the_square}
\end{align*}

%  \+x^⊤ \+M \+x − 2\+b^⊤ \+x = (\+x−\+M^{−1} \+b)^T \+M (\+x−^\+M^{−1} \+b)− \+b^T \+M^{−1} \+b
% 

\subsection{The trace of a matrix product}

The trace of a matrix product behaves like a dot product.  

Let $\+A, \+B \in \R^{m \times n}$.  Then 
%
\begin{align*}
\tr (\+A^T \+B) = \ds\sum_{i=1}^n (\+A^T \+B)_i = \ds\sum_{i=1}^n \ds\sum_{j=1}^m a_{ij} b_{ij}
\labelit \label{eqn:trace_of_matrix_product}
\end{align*}
%
i.e.,  the trace of the matrix product is obtained by summing up the element-wise products. 


\section{Gaussian Facts}

\subsection{Entropy facts about Multivariate Gaussian}

\input{/Users/miw267/Repos/exponential_family/entropy_facts_about_mvn}

\subsection{The simplest linear Gaussian model}

\begin{proposition}
\label{prop:simplest_linear_gaussian_model}
Let
\begin{align*}
X &\sim \N(\+\mu,  \+\Sigma) \\
Y \cond X &\sim \N(\+AX+\+b,  \+V) \\
\end{align*}

Then
\begin{align*}
Y &\sim \N(\+A \+\mu + \+b,  \+V + \+A \+\Sigma \+A^T)
\end{align*}
\end{proposition}

\begin{proof}
See Section 2.3.3 of \cite{bishop2006pattern}.    The necessary computation is for the density

\begin{align*}
p(y)  = \ds\int p(y \cond x) \,  p(x) \,  dx
\labelit \label{eqn:computation_for_the_simplest_linear_gaussian_model}
\end{align*}
\end{proof}


\begin{remark}
The necesary computation in \eqref{eqn:computation_for_the_simplest_linear_gaussian_model} can be seen as the convolution of two Gaussians.
\end{remark}


\begin{remark}
This scheme can be generalized to \textit{linear Gaussian models}.  These are probabilistic graphical models whose conditional distributions are all Gaussian,  with a mean that is a linear function of parents,  and a covariance that is independent of parents.   See Sec 8.1.4 of \cite{bishop2006pattern}, or \cite{roweis1999unifying}.   
\end{remark}


 \subsection{Exponential family representation of Multivariate Gaussian in message passing}
 \label{sec:mvn_in_message_passing}
 
In a dissertation on Gaussian Belief Propagation \cite{bickson2008gaussian}, referred to in \cite{krishnan2016structured}, a multivariate Gaussian is considered as a Markov Random Field. 

In particular, consider the Markov Random field

\begin{equation}
p(x) = \df{1}{Z}\bp{ \ds\prod_{i=1}^n \selfpotential(x_i) \ds\prod_{i,j} \edgepotential(x_i, x_j)  } 
\label{eqn:mrf_dyadic}
\end{equation}

Now note that a multivariate Gaussian has a joint distribution which can be expressed as
\[ p(x) \propto \exp \bigg\{ -\df{1}{2} x^T A x + b^T x \bigg\} \]

as this is just the exponential family form of a Gaussian (e.g., see  \cite{englehardt2013gaussian}), where the natural parameters are
given in terms of the \textit{precision} $\Sigma^{-1}$
\begin{align*}
A &= \Sigma^{-1} \\
b &= \Sigma^{-1} \mu
\end{align*}

Thus, the multivariate Gaussian is a MRF where the potentials in \eqref{eqn:mrf_dyadic} are given by

\begin{align*}
\edgepotential_{ij}(x_i, x_j) & := \exp \bigg\{ -\df{1}{2} x_i A_{ij} x_j \bigg\}  \\
\selfpotential_{i}(x_i) & := \exp \bigg\{ -\df{1}{2} A_{ii} x_i^2 + b_i x_i \bigg\}  
\end{align*}

This seems to be useful in inference for state space models, where one multiplies multiple ``messages" that are different Gaussian densities \textit{over the same variable}.   For example, see  the equations for $\mu_t$ and $\sigma^2_t$ in Section 4 of \cite{krishnan2016structured}, where messages from the past and the future of a time series model are combined to get a posterior distribution on the state $z_t$.   The combined parameters have an expression which may at first be puzzling:

\[ \mu_t = \df{\mu_1 \sigma^2_2 + \mu_2 \sigma^2_1}{\sigma^2_1 + \sigma^2_2}  , \quad \sigma^2_t = \df{\sigma^2_1 \sigma^2_2}{ \sigma^2_1 + \sigma^2_2} \]

However, these messages have an intuitive form when considered in terms of the natural parameterizations:  the combined mean is a weighted combination of the original means, with the weights given by the precisions.   The combined precision (inverse covariance) is given simply by the sum of the original precisions.  Very nice! 

 See Figure \ref{fig:lemma_twelve_bickson} for the general expression, which explains the formula in  \cite{krishnan2016structured}.  This is an example of where the natural parametrization provides more insight than the standard parametrization. 

\begin{figure}[H]
\includegraphics[width=\textwidth]{images/bickson_lemma_12}
\caption{Lemma 12 of \cite{bickson2008gaussian}}
\label{fig:lemma_twelve_bickson}
\end{figure}

\section{The Inverse Wishart Distribution} \label{sec:inverse_wishart_distribution}
 
The Inverse Wishart is a distribution on symmetric,  positive definite matrices.     The Inverse Wishart distribution,  denoted $\InverseWishart(\nu, \+\Psi)$,  has density

\begin{align}
p(\+\Sigma) \propto | \+\Sigma|^{-(\nu+d +1) /2}  \exp \bb{ -\df{1}{2} \tr ( \+\Sigma^{-1} \+\Psi) } 
\label{eqn:inverse_wishart_density}
\end{align}
where $\+\Sigma \succ 0$ and $\nu > d-1$ to have a proper prior.     The expected value of an Inverse Wishart random variable parametrized as in \eqref{eqn:inverse_wishart_density} is given by $\E[\+\Sigma] = \frac{\+\Psi}{\nu -d -1}$.   

\begin{remark}{\remarktitle{Interpreting the parameters of the Inverse Wishart}}
\label{rk:inverse_wishart_parameter_interpretation}
Note that the parameters of the Inverse Wishart can be interpreted (as per conjugacy;  see \eqref{eqn:Sigma_cc_normal_model_cond_conj_prior}) in the following way: the covariance was estimated from $\nu$ observations with a residual sum of squares (a.k.a.  sum of pairwise deviation products) $\+\Psi$. 
 
\end{remark}

Remark \ref{rk:inverse_wishart_parameter_interpretation} also provides inuition on the expected value.      For a visualization of how samples are affected by the parameters,  see \cite{hughes2012inverse}. 

\begin{remark}{\remarktitle{Peter Hoff's notation for the Inverse Wishart: A warning}}
\label{rk:peter_hoffs_notation_for_inverse_wishart}
Note that some authors (e.g.  \cite{hoff2009first},  pp.257) use the notation $\InverseWishart(\nu,  \+M)$ to refer to the density under reparametrization
\begin{align}
p(\+\Sigma) \propto | \+\Sigma|^{-(\nu+d +1) /2}  \exp \bb{ -\df{1}{2} \tr ( \+\Sigma^{-1} \+M^{-1}) } 
\label{eqn:inverse_wishart_density_hoff}
\end{align}
and therefore appropriately altered normalization constant.  The expected value of an Inverse Wishart random variable parametrized as in   \eqref{eqn:inverse_wishart_density_hoff},  is given by $\E[\+\Sigma] = \frac{\+M^{-1}}{\nu -d -1}$.     

However,  Hoff later introduces the reparametrization $\+S := \+M^{-1}$,  and so writes $\InverseWishart(\nu, \+S^{-1})$ to mean 
\begin{align}
p(\+\Sigma) \propto | \+\Sigma|^{-(\nu+d +1) /2}  \exp \bb{ -\df{1}{2} \tr ( \+\Sigma^{-1} \+S) } 
\label{eqn:inverse_wishart_density_hoffs_alternate}
\end{align}
which is \eqref{eqn:inverse_wishart_density}, and which we would write as $\InverseWishart(\nu, \+S)$.
 
\end{remark}

\begin{remark}{\remarktitle{Comparing parametrizations}}
Regarding Remark \ref{rk:peter_hoffs_notation_for_inverse_wishart},  prefer our notation because 
\begin{itemize}
\item It is the natural parameterization (see Example \ref{ex:inverse_wishart_as_ef}).
\item It lets $\+\Psi$ be interpreted directly as a prior residual sum of squares (see Remark \ref{rk:inverse_wishart_parameter_interpretation}). 
\item It matches the parametrization used throughout Wikipedia,  e.g. in its conjugacy tables.
\end{itemize}
 
\end{remark}



\subsection{Relation to other distributions}
The Wishart distribution has the same support as the Inverse Wishart;  however,  the Wishart does not give a conditionally conjugate prior on the covariance of a normal distribution.    The Inverse Wishart density can be derived from the Wishart via the multivariate change of variables  \cite{wolpert2011change}. \footnote{It is claimed in Wikipedia that if $\+X \sim \Wishart(\nu,  \+\Psi)$  then $\+X^{-1} \sim \InverseWishart(\nu,  \+\Psi^{-1})$.   I almost was able to show this using the multivariate change of variables  \cite{wolpert2011change} along with (15.15) of \cite{dwyer1967some},  but I  was off by a negative when attempting to combine the two terms with $|\+X^{-1}|$ raised to an exponent.}\redfootnote{TO DO: Provide derivation.}

In particular,  we have the relation $\+\Sigma \sim \InverseWishart(\nu, \+\Psi) \implies \+\Sigma^{-1} \sim \Wishart(\nu,  \+\Psi^{-1})$.    Thus,  if covariance matrix $\+\Sigma$ has this Inverse Wishart distribution,  then we obtain the expected value of the precision matrix as $\E[\+\Sigma^{-1}] = \nu \+\Psi^{-1}$.

The inverse Wishart can be seen as a generalization of the inverse gamma distribution to multiple dimensions.\redfootnote{TODO: fill in.   Make explicit how it is a generalization.}


\subsection{Entropy and relative entropy}

Let $\+\Sigma$ have an Inverse Wishart distribution (parametrized to have density \eqref{eqn:inverse_wishart_density}).  Then its entropy is given by \cite{gupta2010parametric}:

\begin{align*}
\H(\+\Sigma) = \ln \Gamma_d \bigg(\frac{\nu}{2} \bigg) + \frac{\nu d}{2}
+ \frac{d+1}{2} \ln \bigg| \frac{\+\Psi}{2} \bigg| - \frac{\nu + d+ 1}{2} \sum_{i=1}^d \psi \bigg( \frac{\nu - d + i}{2} \bigg)
\end{align*}
where $\psi$ denotes the digamma function,  $\psi(x) = \frac{d}{dx} \Gamma(x)$.\redfootnote{It is unfortunate that in our notation,  $\+\Psi$ and $\psi$ mean completely different things;  fix this.}

The relative entropy between two Inverse Wishart distributions $p_1, p_2$ with parameters $\nu_1,  \+\Psi_1$ and $\nu_2,  \+\Psi_2$ is given by \cite{gupta2010parametric}:

\begin{align*}
\KL{p_1}{p_2} &= \ln \bigg( \df{\Gamma_d ( \frac{\nu_2}{2})}{\Gamma_d (\frac{\nu_1}{2})} \bigg) + \df{\nu_1}{2} \tr(\+\Psi_1^{-1} \+\Psi_2) - \df{\nu_1 d}{2} - \df{\nu_2}{2} \ln \bigg| \+\Psi_1^{-1} \+\Psi_2 \bigg| - \df{\nu_2 - \nu_1}{2}\sum_{i=1}^d \psi \bigg( \frac{\nu_1 - d + i}{2}  \bigg)
\end{align*}

\subsection{Sampling}

A sample $\+\Sigma$ from the $\InverseWishart(\nu, \+\Psi)$ distribution (using the natural parametrization of \eqref{eqn:inverse_wishart_density}) can be obtained by the following scheme\redfootnote{TODO: Provide derivation of this scheme.  See perhaps \url{https://www.math.wustl.edu/~sawyer/hmhandouts/Wishart.pdf}.} \cite{hoff2009first}:

\begin{enumerate}
\item Sample $\+z_1, ..., \+z_{\nu} \iid \N (\+0,  \+\Psi^{-1})$
\item Calculate $\+Z^T \+Z = \sum_{i=1}^\nu \+z_i \+z_i^T$.
\item Set $\+\Sigma = ( \+Z^T \+Z )^{-1}$.
\end{enumerate}
The intuition is that the Inverse Wishart models covariance matrices as an inverse sum of squares (again, see Remark \ref{rk:inverse_wishart_parameter_interpretation}).  

\subsection{Evaluation as a model for covariance matrices}

The  Inverse Wishart is a popular choice for modeling covariance matrices (e.g.  see \cite{hoff2009first}),  due to at least the fact that is a  conditionally conjugate prior on the covariance of a normal distribution.   (See Section \ref{sec:normal_data_with_non_conjugate_prior}.)   It seems to me that a weakly informative prior could be constructed by setting  $\nu = d+2$ (the smallest integer for which $\nu$ is in the parameter space) and $\+\Psi = (\nu -d - 1) \+I = \+I$.   This would presumably be reasonable at least if one expected unit variances and wanted to make a prior assumption of independence across dimensions.

Some problems with the Inverse Wishart as a model for covariance matrices is summarized in  \cite{alvarez2014bayesian}.    We highlight that:
\begin{enumerate}
\item  When $\nu  > 1$,   the implied scaled inv-$\chi^2$ distribution on the individual variances has extremely low density in the region near zero.   
\item The prior imposes a dependency between the correlations and the variances. In particular,  larger variances are associated with absolute values of the correlations near 1 while small variances
are associated with correlations near zero.    
\end{enumerate}
For additional discussion on the problems with Inverse Wishart,  especially when used in hierarchical models,  and for a remedy using a half-t distribution that also has a conditionally conjugate construction,  see \cite{wojnowicz2022categorical}.


\section{General Conjugacy Formalism: Alternate Approaches}

\subsection{General Conjugacy Formalism: Alternate Approach 1\footnote{This argument follows the argument (and notation) of \cite{johnson2016composing}, Appendix B.  As of now, I find it more intuitive then the argument given in the main body.}}
\label{sec:general_conjugacy_alternate_argument}


\input{inputs/conjugacy_general_composing_approach.tex}

Note that this argument yields the same parameter updating scheme of \eqref{eqn:general_formalism_prior_to_posterior_conversion}.


\subsection{Alternate approach 2} \label{sec:ef_general_formalism_jordan}

Following \cite{jordan2010ef}, here we provide a general formalism for conjugate priors for exponential family data models. 

Writing the exponential family density in canonical form, we have
\[ p(x \cond \eta) = h(x) \exp \{ \eta^T T(x) - A(\eta) \} \]
where $\eta$ is the canonical parameter, $T(x)$ are the sufficient statistics,  $h(x)$ is the carrier density, and $A(\eta)$ is the log normalizer (and so is \textit{not} a degree of freedom). 

The natural parameter space is 
\[  \bigg\{\eta : \ds\int h(x) \exp \{ \eta^T T(x) - A(\eta) \} < \infty \bigg \}\]

Given a random sample, $\+x=(x_1, x_2, .., x_N)$, we obtain:
\[ p(\+x \cond \eta) = \bigg( \ds\prod_{i=1}^N h(x_i)  \bigg) \exp \bigg\{ \eta^T  \ds\sum_{i=1}^N T(x_i) - N A(\eta) \bigg\} \]
as the likelihood function.

A conjugate prior can be obtained by mimicking the likelihood
\begin{equation}
p (\eta \cond \tau, n_0) = H(\tau, n_0) \exp \{ \tau^T \eta - n_0 A(\eta)\}
\label{eqn:conjugate_prior_exptl_family}
\end{equation}

where now $H(\tau, n_0)$ is the normalizing factor.  (For conditions on normalizability, see \cite{jordan2010ef}).   Note that $\tau$ has the dimensionality of the canonical parameter $\eta$ and $n_0$ is a scalar.

To verify conjugacy, we compute the posterior density
\[ p (\eta \cond \+x, \tau, \eta_0)  \propto \exp \bigg\{ \bigg( \tau + \ds\sum_{n=1}^N  T(x_n) \bigg)^T \eta - (n_0 + N) A(\eta) \bigg\} \]
which retains the form of \eqref{eqn:conjugate_prior_exptl_family}.  For an argument that is perhaps more intuitive, see Section \ref{sec:general_conjugacy_alternate_argument} 

Thus, the prior-to-posterior conversion can be summarized with the following update rules
\begin{align*}
\tau & \to \tau + \ds\sum_{n=1}^N T(x_n) \\
n_0 & \to  n_0 + N \\
\labelit \label{eqn:general_formalism_prior_to_posterior_conversion}
\end{align*}

For conjugate Bayesian models, the predictive posterior distribution, $p(x_{\text{new}} \cond x)$ is always tractable, because it has the same form (integrating a likelihood against the parameter distribution) as does the evidence term in Bayes law.   For exponential family models, the predictive posterior takes the form of a ratio of normalizing factors

\begin{equation}
p(x_{\text{new}} \cond x) = \df{H(\tau_{\text{post}}, n_0 + N)}{H(\tau_{\text{post}} + T(x_{\text{new}}), n_0 + N + 1)}
\label{eqn:predictive_posterior_exptl_family}
\end{equation}

\red{TODO:  Redo some of the examples using the exponential family conjugate prior formalism.    A possibly useful resource in the giant table at \url{https://en.wikipedia.org/wiki/Exponential_family}.}

\section{Bayesian networks} \label{sec:bayesian_networks}


\subsection{Overview}

\input{/Users/mwojno01/Repos/minis/pgms/bayesian_network}



\subsection{Markov blankets} \label{sec:markov_blankets}

\input{/Users/mwojno01/Repos/minis/pgms/markov_blankets_and_complete_conditionals}




\section{Bayesian multivariate linear regression: Alternate derivations}  \label{sec:more_bayesian_multivariate_linear_regression} 


Below we give two alternate proofs for the posterior of the regression weights in multivariate linear regression,  compared to what was given in Proposition \ref{prop:bayes_linear_regression_with_known_ssq}.  

We begin by restating the proposition here,  then we provide the alternate proof.

\begin{proposition} \label{prop:bayes_linear_regression_with_known_ssq}
Consider the Bayesian linear multiple regression model with known observation noise $\sigma^2$
\begin{align*}
\+\beta &\sim \N (\+\mu_0, \+\Sigma_0) \\
y_i \cond \+\beta &\indsim \N (\+x_i^T \+\beta, \sigma^2),  \quad i=1,...,n \\
\labelit \label{eqn:bayesian_multiple_linear_regression_with_known_observation_noise_APPENDIX}
\end{align*}
where  $\+x_i$ designates the $i$-th row of the design matrix $\+X \in \R^{n \times p}$.

The posterior distribution for \eqref{eqn:bayesian_multiple_linear_regression_with_known_observation_noise} is given by 
\begin{align*}
\+\beta \cond \+y & \sim \N(\+\mu,  \+\Sigma )
\intertext{where}
\+\Sigma &= \bp{\+\Sigma_0^{-1} +  \frac{1}{\sigma^2} \+X^T \+X }^{-1}  \\
\+\mu &= \+\Sigma \bp{   \+\Sigma_0^{-1} \+\mu_0 +  \frac{1}{\sigma^2} \+X^T  \+y}  \\
\labelit \label{eqn:posterior_bayesian_lin_regression_with_known_obs_var_APPENDIX}
\end{align*}

\end{proposition}


\begin{proof}
By Bayes rule,
\begin{align*} 
p(\+\beta \cond \+y) &\propto   p(\+\beta)  \exp \bigg\{  \ds\sum_{i=1}^N  -\frac{1}{2 \sigma^2} \bigg( y_i -  \+x_i^T \+\beta \bigg)^2  \bigg\} \\
\intertext{and defining $\+\Omega  \in \R^{n \times n}: \+\Omega = \text{diag} (\frac{1}{\sigma^2}, ..., \frac{1}{\sigma^2})$,  we have} 
&\stackrel{1}{\propto}  p(\+\beta)   \exp \bigg\{  -\half  (\+y - \+X \+\beta)^T \+\Omega   (\+y - \+X \+\beta) \bigg\} \\ 
&\stackrel{2}{\propto}  p(\+\beta)   \exp \bigg\{  -\half  (\+X^+ \+y - \+\beta)^T  \+X^T \+\Omega \+X   (\+X^+ \+y - \+\beta) \bigg\} 
\end{align*}
where (1) writes the weighted sum of squares in matrix notation, and (2) isolates $\+\beta$,  using $\+X^{+}$,  the Moore-Penrose psuedo-inverse of $\+X$. \footnote{Specifically,  since $\+X\+X^+ = \+I$,  we use 
\begin{align*}
(\+y - \+X \+\beta)^T \+\Omega   (\+y - \+X \+\beta) &= (\+X\+\beta - \+y)^T \+\Omega (\+X\+\beta - \+y)\\ &= \bigg( \+X (\+\beta - \+X^+ \+y ) \bigg)^T \+\Omega  \bigg(  \+X (\+\beta - \+X^+ \+y ) \bigg) \\
&= (\+\beta - \+X^+ \+y)^T \+X^T \+\Omega \+X   (\+\beta - \+X^+ \+y) 
\end{align*}
.}  

Thus,  we see that $p(\+\beta \cond  \+y)$ is proportional to the product of two multivariate Gaussians:  $p(\+\beta)$,  which has mean $\+\mu_0$ and covariance $\+\Sigma_0$,  and another Gaussian,  which has mean $\+X^+ \+y$ and covariance $(\+X^T \+\Omega \+X)^{-1}$.   We know from the exponential family representation of the Gaussian that the resulting distribution can be obtained by summing at the scale of natural parameters -- which for the Gaussian are the precision and precision-weighted mean. \footnote{See,  for reference,  Section \ref{sec:mvn_in_message_passing}.}   Using this,  we obtain

\begin{align*}
p(\+\beta \cond \+y) & \sim \N(\+\mu,  \+\Sigma )
\intertext{where}
\+\Sigma &= \bp{\+\Sigma_0^{-1} +  \+X^T \+\Omega \+X }^{-1}  \\
\+\mu &= \+\Sigma  \bp{   \+\Sigma_0^{-1} \+\mu_0 + \+X^T \+\Omega \cancel{\+X} \cancel{\+X^+}  \+y }  \\
&= \+\Sigma \bp{   \+\Sigma_0^{-1} \+\mu_0 + \+X^T \+\Omega \+y}  
\end{align*}

recalling that we defined $\+\Omega = \text{diag} (\frac{1}{\sigma^2}, ..., \frac{1}{\sigma^2})$ completes the proof. 
\end{proof}


Now we provide a third proof,  which may be of interest.   Whereas both proofs of proposition \ref{prop:bayes_linear_regression_with_known_ssq} that we have seen so far (both the proof given in Section  \ref{sec:Bayesian_linear_regression_with_normal_prior}, as well as the one given immediately above)  refer to exponential family properties,  the proof below does not,  and instead uses multivariate completing the square to do the heavy lifting. 

Note that proposition 
\ref{prop:bayes_linear_regression_with_known_ssq_and_zero_prior_mean} below, as stated,  is slightly more restrictive in that it assumes the prior mean is zero.   This additional restriction is not necessary; the proposition could be rewritten to match 
Proposition \ref{prop:bayes_linear_regression_with_known_ssq} exactly,  and the proof could be adjusted accordingly to match the additional generality.    The difference in statements is just an unnecessary presentational blemish.\redfootnote{TODO: fix up the unnecessary presentational blemish -- assuming that we don't end up sacrifing too much pedagogical clarity for the sake of generality}.
%The posterior distribution for \eqref{eqn:bayesian_multiple_linear_regression_with_known_observation_noise} is given by 


\begin{proposition} \label{prop:bayes_linear_regression_with_known_ssq_and_zero_prior_mean}
Consider the Bayesian linear multiple regression model
\begin{align*}
\+\beta &\sim \N (\+0, \+V) \\
y_i \cond \+\beta &\indsim \N (\+x_i^T \+\beta, \sigma^2),  \quad i=1,...,n
\end{align*}

where  $\+x_i$ designates the $i$-th row of the design matrix $\+X \in \R^{n \times p}$.

The posterior distribution for this model is given by 
\begin{align*}
p(\+\beta \cond \+y) & \sim \N(\+\mu,  \+\Sigma )
\intertext{where}
\+\mu &= \df{1}{\sigma^2} \+\Sigma \+X^T \+y \\
\+\Sigma &= \bigg( \df{1}{\sigma^2}  \+X^T \+X + \+V^{-1}  \bigg)^{-1} \\
\end{align*}
\end{proposition} 


\begin{proof}
The posterior on $\+\beta$ given $\+y = (y_1,...,y_N)^T$ is Gaussian,  since

\begin{align*}
\ln p(\+\beta \cond \+y) &= \ds\sum_{i=1}^n \ln p(y_i \cond \+\beta)  + \ln p(\+\beta) \\
&= -\df{1}{2 \sigma^2} \ds\sum_{i=1}^n (y_i - \+x_i^T \+\beta)^2  - \half \+\beta^T \+V^{-1} \+\beta + \text{constant} \\
&\stackrel{1}{=} -\df{1}{2 \sigma^2} \bigg(  \+y^T \+y - 2 \+y^T \+X^T \+\beta + \+\beta^T \+X^T \+X \+\beta \bigg)  - \half \+\beta^T \+V^{-1} \+\beta + \text{constant} \\
&\stackrel{2}{=} -\half (\+\beta - \+\mu)^T \+\Sigma^{-1} (\+\beta - \+\mu) +  \text{constant} \\
\intertext{where}  \\
\+\mu &= \df{1}{\sigma^2} \+\Sigma \+X^T \+y \\
\+\Sigma &= \bigg( \df{1}{\sigma^2}  \+X^T \+X + \+V^{-1}  \bigg)^{-1} \\
\end{align*}

Equality (1) is obtained by noting $ \sum_{i=1}^n (y_i - \+x_i^T \+\beta)^2  = (\+y - \+X \+\beta)^T (\+y - \+X \+\beta)$, FOIL-ing, and observing that the cross-products are scalars.   Equality (2) is obtained by completing the square,  where $\+\beta$ plays the role of $\+x$  in \eqref{eqn:multivariate_completing_the_square},  and where in that notation we have $\+M = \frac{1}{\sigma^2} \+X^T \+X + \+V^{-1}$ and $\+b^T = \frac{1}{\sigma^2} \+y^T \+X^T$.
\end{proof}


\end{document}